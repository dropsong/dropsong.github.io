<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习小记 | dropsong's</title><meta name="keywords" content="AI,神经网络,深度学习,TensorFlow,推荐系统,CNN,图像风格变换,论文阅读,Embedding,RNN,LSTM,GPU"><meta name="author" content="dropsong"><meta name="copyright" content="dropsong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前置知识请见 人工智能导论 。 初识 TensorFlow官网： https:&#x2F;&#x2F;www.tensorflow.org TensorFlow 大框架：  Estimator 属于 High level 的 API，而 Mid-level API 分别是：  Layers：用来构建网络结构 Datasets：用来构建数据读取 pipeline Metrics：用来评估网络性能  Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习小记">
<meta property="og:url" content="https://dropsong.github.io/posts/eeca0305.html">
<meta property="og:site_name" content="dropsong&#39;s">
<meta property="og:description" content="前置知识请见 人工智能导论 。 初识 TensorFlow官网： https:&#x2F;&#x2F;www.tensorflow.org TensorFlow 大框架：  Estimator 属于 High level 的 API，而 Mid-level API 分别是：  Layers：用来构建网络结构 Datasets：用来构建数据读取 pipeline Metrics：用来评估网络性能  Tensorflow">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2025/05/16/4BTsNnSgIPFewCh.jpg">
<meta property="article:published_time" content="2025-05-16T06:57:13.000Z">
<meta property="article:modified_time" content="2025-07-31T10:04:08.356Z">
<meta property="article:author" content="dropsong">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="TensorFlow">
<meta property="article:tag" content="推荐系统">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="图像风格变换">
<meta property="article:tag" content="论文阅读">
<meta property="article:tag" content="Embedding">
<meta property="article:tag" content="RNN">
<meta property="article:tag" content="LSTM">
<meta property="article:tag" content="GPU">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/05/16/4BTsNnSgIPFewCh.jpg"><link rel="shortcut icon" href="/img/favicon2.jpg"><link rel="canonical" href="https://dropsong.github.io/posts/eeca0305"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习小记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-31 18:04:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="dropsong's" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/11/07/tEWlYGuVUqFvxw7.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">106</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">161</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2025/05/16/4BTsNnSgIPFewCh.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">dropsong's</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习小记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-05-16T06:57:13.000Z" title="发表于 2025-05-16 14:57:13">2025-05-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.6k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>前置知识请见 <a href="https://dropsong.github.io/posts/6f3f8819.html">人工智能导论</a> 。</p>
<h1 id="初识-TensorFlow"><a href="#初识-TensorFlow" class="headerlink" title="初识 TensorFlow"></a>初识 TensorFlow</h1><p>官网： <a target="_blank" rel="noopener" href="https://www.tensorflow.org">https://www.tensorflow.org</a></p>
<p>TensorFlow 大框架：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/16/MThykvKcuZ3SPEW.png" alt="106-1.png"></p>
<p>Estimator 属于 High level 的 API，而 Mid-level API 分别是：</p>
<ul>
<li>Layers：用来构建网络结构</li>
<li>Datasets：用来构建数据读取 pipeline</li>
<li>Metrics：用来评估网络性能</li>
</ul>
<p>Tensorflow1.0 主要特性：</p>
<ul>
<li>XLA —— Accelerate Linear Algebra<ul>
<li>提升训练速度 58 倍</li>
<li>可以在移动设备运行</li>
</ul>
</li>
<li>引入更高级别的 API —— <code>tf.layers</code> / <code>tf.metrics</code> / <code>tf.losses</code> / <code>tf.keras</code></li>
<li>Tensorflow 调试器</li>
<li>支持 docker 镜像，引入 tensorflow serving 服务</li>
</ul>
<p>Tensorflow2.0 主要特性：</p>
<ul>
<li>使用 <code>tf.keras</code> 和 <code>eager mode</code> 进行更加简单的模型构建</li>
<li>鲁棒的跨平台模型部署</li>
<li>强大的研究实验</li>
<li>清除不推荐使用的 API 和减少重复来简化 API</li>
</ul>
<p>使用 Tensorflow 的大致流程：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/16/Dxpi3b5RuLkQmsB.png" alt="106-2.png"></p>
<p>在虚拟环境中安装 tensorflow 。</p>
<p>一个 helloword 程序：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;*******************&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="number">0.</span>) <span class="comment"># 常量  张量</span></span><br><span class="line">y = tf.constant(<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    x = x+y</span><br><span class="line">    y = y/<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.numpy())  <span class="comment"># 张量和 ndarray 之间可以互相转化</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p>在我本地没有 nvidia GPU 的电脑，输出类似于：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 前面还有一大堆警告和报错...</span><br><span class="line">This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.</span><br><span class="line">To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class="line">*******************</span><br><span class="line">2.19.0</span><br><span class="line">[时间]E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)</span><br><span class="line">2.0</span><br><span class="line">tf.Tensor(2.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>在 Colab 中运行该代码，得到输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*******************</span><br><span class="line">2.18.0</span><br><span class="line">2.0</span><br><span class="line">tf.Tensor(2.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>另一个例子：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;*******************&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.</span>) <span class="comment"># 变量  张量</span></span><br><span class="line">y = tf.Variable(<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line">x.assign(x+y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">前略</span></span><br><span class="line"><span class="string">*******************</span></span><br><span class="line"><span class="string">2.19.0</span></span><br><span class="line"><span class="string">[时间]: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)</span></span><br><span class="line"><span class="string">&lt;tf.Variable &#x27;Variable:0&#x27; shape=() dtype=float32, numpy=0.0&gt;</span></span><br><span class="line"><span class="string">&lt;tf.Variable &#x27;Variable:0&#x27; shape=() dtype=float32, numpy=1.0&gt;</span></span><br><span class="line"><span class="string">&lt;tf.Variable &#x27;Variable:0&#x27; shape=() dtype=float32, numpy=1.0&gt;</span></span><br><span class="line"><span class="string">&lt;tf.Variable &#x27;Variable:0&#x27; shape=() dtype=float32, numpy=1.0&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><strong>图结构</strong>：静态图，效率高；动态图，调试容易。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/16/pezBNM4RgjmLASC.png" alt="106-3.png"></p>
<p><strong>图（graph）</strong> 是 tensorflow 用于表达计算任务的一个核心概念。从前端（Python）描述神经网络的结构，到后端在多机和分布式系统上部署，到底层 Device（CPU、GPU、TPU）上运行，都是基于图来完成。</p>
<p>数据流图：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/19/cwaknUeL43vq9AX.png" alt="106-4.png"></p>
<p>张量为什么那么多阶？<br>可以考虑一些典型场景。例如，灰度图、png、视频等。</p>
<p><strong>深度学习中的“张量”</strong> 是工程实践中使用的多维数组结构，而 <strong>数学中的张量</strong> 是抽象代数与几何的核心概念，两者在形式上相似，但本质和用途<strong>大不相同</strong>。</p>
<p>张量属性：</p>
<ul>
<li><code>graph</code> 张量所属的默认图</li>
<li><code>op</code> 张量的操作名</li>
<li><code>name</code> 张量的字符串描述</li>
<li><code>shape</code> 张量形状</li>
</ul>
<h1 id="Tensorflow-keras"><a href="#Tensorflow-keras" class="headerlink" title="Tensorflow-keras"></a>Tensorflow-keras</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>keras 是基于 python 的高级神经网络 API，以 Tensorflow、CNTK 或 Theano 为后端运行，keras 必须有后端才可以运行。后端可以切换，现在多用 tensorflow. 方便快速实验，帮助用户以最少的时间验证自己的想法。</p>
<p>Tensorflow-keras 是什么？</p>
<ul>
<li>Tensorflow 对 keras API 规范的实现</li>
<li>相对于以 tensorflow 为后端的 keras, Tensorflow-keras 与 Tensorflow 结合更加紧密</li>
<li>实现在 <code>tf.keras</code> 空间下</li>
</ul>
<p>Tf-keras 和 keras 联系：</p>
<ul>
<li>基于同一套 API</li>
<li>keras 程序可以通过修改导入方式轻松转为 <code>tf.keras</code> 程序</li>
<li>反之可能不成立，因为 <code>tf.keras</code> 有其他特性</li>
<li>相同的 JSON 和 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104145585">HDF5</a> 模型序列化格式和语义</li>
</ul>
<h2 id="tf-keras做分类"><a href="#tf-keras做分类" class="headerlink" title="tf_keras做分类"></a>tf_keras做分类</h2><p>下面的 ipynb 文件包含以下内容：</p>
<ul>
<li>一个衣服分类的实验</li>
<li>为什么要分为训练集，验证集，测试集</li>
<li>tensorflow 的基本使用</li>
<li>均方误差</li>
<li>Cross Entropy Loss Function（交叉熵损失函数）</li>
<li>w 的初始分布</li>
</ul>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/tf_keras%E5%81%9A%E5%88%86%E7%B1%BB/tf01_keras_classification.ipynb" ></iframe>

<p>下面的 ipynb 文件包含以下内容：</p>
<ul>
<li>对之前相同的流程，做归一化，看看有无改进</li>
</ul>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/tf_keras%E5%81%9A%E5%88%86%E7%B1%BB/tf02_keras_classification_model-normalize.ipynb" ></iframe>

<p>下面的 ipynb 文件包含以下内容：</p>
<ul>
<li>TensorBoard</li>
<li>ModelCheckpoint</li>
<li>EarlyStopping</li>
</ul>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/tf_keras%E5%81%9A%E5%88%86%E7%B1%BB/tf03_keras_classification_model-callbacks.ipynb" ></iframe>

<h2 id="tf-keras做回归"><a href="#tf-keras做回归" class="headerlink" title="tf_keras做回归"></a>tf_keras做回归</h2><p>下面的 ipynb 文件包含以下内容：</p>
<ul>
<li>使用 California Housing dataset 做回归</li>
</ul>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/tf_keras%E5%81%9A%E5%9B%9E%E5%BD%92/tf04_keras_regression.ipynb" ></iframe>

<h1 id="梯度消失、梯度爆炸"><a href="#梯度消失、梯度爆炸" class="headerlink" title="梯度消失、梯度爆炸"></a>梯度消失、梯度爆炸</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>关于梯度消失，从一个例子入手：</p>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/tf05_keras_classification_model-dnn.ipynb" ></iframe>

<p>上面文件演示的问题即是梯度消失。</p>
<p><strong>梯度消失</strong>：<br>梯度消失问题发生时，接近于输出层的 hidden layer 3 等的权值更新相对正常，但前面的 hidden layer 1 的权值更新会变得很慢，导致<strong>前面的层权值几乎不变，仍接近于初始化的权值</strong>，这就导致 hidden layer 1 相当于只是一个映射层，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。</p>
<p><strong>梯度爆炸</strong>：<br>在反向传播过程中使用的是链式求导法则，如果每一层偏导数都大于 1，那么连乘起来将以指数形式增加，误差梯度不断累积，就会造成梯度爆炸。梯度爆炸会导致模型权重更新幅度过大，会造成<strong>模型不稳定，无法有效学习</strong>，还会出现<strong>无法再更新的 NaN 权重值</strong>。</p>
<p>梯度消失和梯度爆炸其实都是因为<strong>反向传播中的连乘效应</strong>。除此之外，也和<strong>激活函数的选取</strong>有关。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/22/NRDOnXCu6W1qf9y.png" alt="106-5.png"></p>
<script type="math/tex; mode=display">
\text{selu}(x) = \lambda \begin{cases} 
x & \text{if } x > 0 \\
\alpha e^x - \alpha & \text{if } x \leq 0 
\end{cases}</script><p>对于梯度消失：</p>
<ul>
<li>Sigmoid 函数的导数最大值是 0.25（当 $ z = 0 $ 时）。当 $ z $ 的绝对值较大时（即神经元饱和时），$ \sigma’(z) $ 接近于 0 .</li>
<li>Tanh 函数，当 $ z $ 的绝对值较大时，导数也接近于 0 .</li>
<li>在反向传播过程中，如果<strong>许多层的激活函数导数值都小于1</strong>（特别是当它们远小于1时，比如 Sigmoid 导数的最大值0.25），这些导数值连乘起来会使梯度呈指数级衰减。</li>
<li>如果<strong>权重被初始化为较小的值</strong>（例如，绝对值小于 1），多个较小的权重矩阵与较小的激活函数导数连乘，会进一步加速梯度的消失。</li>
</ul>
<p>对于梯度爆炸，可以作类似的解释，不再赘述。</p>
<h2 id="批归一化"><a href="#批归一化" class="headerlink" title="批归一化"></a>批归一化</h2><blockquote>
<p>接下来的行文逻辑：什么是批归一化，批归一化的动机，减均值除以标准差为什么失败，正确的批归一化算法。</p>
</blockquote>
<p>批归一化可以缓解梯度消失和梯度爆炸的问题，简单来说即是，每层的激活值都做归一化。</p>
<p>在深度神经网络的训练过程中，每一层的参数都会在反向传播后进行更新。这意味着，对于网络中的某一层，其输入数据的分布（来自前一层的输出）会随着训练的进行而不断发生变化。这种现象被称为<strong>内部协变量偏移 (Internal Covariate Shift, ICS)</strong>。</p>
<p>ICS 会导致以下问题：</p>
<ul>
<li>训练速度减慢：后续层需要不断适应这种变化的输入分布。</li>
<li>对初始化敏感：网络对参数的初始值更加敏感。</li>
<li>激活函数饱和：输入数据分布的变化可能导致激活函数的输入落入饱和区（例如Sigmoid的两端），从而导致梯度消失。</li>
</ul>
<p><strong>批归一化的提出就是为了缓解 ICS 问题。</strong></p>
<p>如何实现批归一化呢？我们自然想到经典的“减均值除以标准差”方法：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/22/TNWJeRn7FvzGKUI.png" alt="106-6.png"></p>
<p>这有两个好处：</p>
<ol>
<li>避免分布数据偏移</li>
<li>远离导数饱和区</li>
</ol>
<p>但这个处理对于在 -1~1 之间的梯度变化不大的激活函数，效果不仅不好，反而更差。比如 sigmoid 函数，在 -1~1 之间几乎是线性，BN 变换后就没有达到非线性变换的目的；而对于 relu，效果会更差，因为会有一半的置零。总之，<strong>减均值除以标准差后可能削弱网络的性能</strong>。</p>
<p>改进的思路是：缩放加移位。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/22/a2rdN3DEOQfVFie.png" alt="106-7.png"></p>
<p>这里的 $ y_i $ 就是批归一化层的输出，它将作为下一层或激活函数的输入。$\gamma$ 和 $\beta$ 是与网络的其他参数（如权重 $ W $ 和偏置 $ b $）一样通过梯度下降学习得到的。如果网络发现原始的表示更好，它可以学习到 $\gamma = \sqrt{\sigma_B^2 + \epsilon}$ 和 $\beta = \mu_B$，从而在一定程度上抵消归一化的影响。</p>
<p>tensorflow 中的接口：<br><a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a></p>
<p>下面是使用批归一化的例子：</p>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/tf06_keras_classification_model-dnn-bn.ipynb" ></iframe>

<h2 id="更改激活函数"><a href="#更改激活函数" class="headerlink" title="更改激活函数"></a>更改激活函数</h2><p><code>relu -&gt; selu</code>, 缓解梯度消失。</p>
<p>实例：</p>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/tf07_keras_classification_model-dnn-selu.ipynb" ></iframe>

<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/22/jkhlRfGuJETvSgF.png" alt="106-8.png"></p>
<p>Dropout 可以防止过拟合，实现也非常简单：对想要 dropout 的点输出乘 0 .</p>
<p>在 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout">tensorflow keras 中的实现</a>：</p>
<blockquote>
<p>The <code>Dropout</code> layer randomly sets input units to 0 with a frequency of <code>rate</code> at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by <code>1 / (1 - rate)</code> such that the sum over all inputs is unchanged.</p>
</blockquote>
<p>实例：</p>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/tf08_keras_classification_model-dnn-selu-dropout.ipynb" ></iframe>

<h1 id="Wide-amp-Deep-模型（推荐场景）"><a href="#Wide-amp-Deep-模型（推荐场景）" class="headerlink" title="Wide &amp; Deep 模型（推荐场景）"></a>Wide &amp; Deep 模型（推荐场景）</h1><p>原始论文:  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.07792v1">https://arxiv.org/pdf/1606.07792v1</a></p>
<p>Google 于 16 年发布，用于分类和回归。曾应用在 Google Play 中的应用推荐。</p>
<p>Wide 用于记忆，Deep 用于联想。<strong>一个推荐系统的经典矛盾：应该给用户推荐经常买的，还是探索一些用户可能会买的？</strong></p>
<p>相关文章：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Yasin0/article/details/100736420">https://blog.csdn.net/Yasin0/article/details/100736420</a> （已备份）</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/57247478">https://zhuanlan.zhihu.com/p/57247478</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/139358172">https://zhuanlan.zhihu.com/p/139358172</a> （已备份）</li>
</ul>
<p>尝试搭建简单的 wide and deep 模型（<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/wide_and_deep/tf09_keras_regression-wide_deep.ipynb">github 链接</a>）：</p>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/wide_and_deep/tf09_keras_regression-wide_deep.ipynb" ></iframe>

<p>在 Deep 部分，会用到 Word2Vec 技术。简要介绍：</p>
<blockquote>
<p>Word2Vec is a technique in natural language processing that represents words as vectors in a high-dimensional space, capturing semantic relationships between them. It essentially translates words into numerical representations, where words with similar meanings are located closer together in the vector space. </p>
</blockquote>
<p>可供参考的资料：<br><a target="_blank" rel="noopener" href="https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673">https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673</a></p>
<p>Word2Vec utilizes two main architectures:</p>
<ul>
<li>CBOW (Continuous Bag of Words): Predicts a target word based on its surrounding context words.</li>
<li>Skip-gram: Predicts surrounding words based on a given input word. </li>
</ul>
<p>简要的原理介绍：<br>如何衡量两个词在语义上是否相近？如果两个词的上下文相同（相近），我们可以有较大的把握认为，这两个词的语义也相近。我们构造一个神经网络，输入为上下文，输出为该词，训练之，将隐藏层的数值作为这个词的 vector 元素。在实践中，我们发现语义相近的词，由这种机制产生的 vector 表示也相近。</p>
<p>有了 Word2Vec，我们就可以用向量之间的差距来衡量信息之间的差距。</p>
<p>用面向对象的方法重写之前的代码：<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/wide_and_deep/tf10_keras_regression-wide_deep-subclass.ipynb">github链接</a>。</p>
<p>在之前的模型中，我们只有一个输入，现在尝试多输入（<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/wide_and_deep/tf11_keras_regression-wide_deep-multi-input.ipynb">github链接</a>）：</p>
<iframe frameBorder="0" width="100%" height="100%" style="min-width: 300px; min-height:600px" src="https://nbviewer.org/github/dropsong/dl-ipynb-examples/blob/master/wide_and_deep/tf11_keras_regression-wide_deep-multi-input.ipynb" ></iframe>

<p>iframe 嵌多了可能有点卡，之后只放 github 链接了。</p>
<p>尝试搭建更加复杂的模型（<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/wide_and_deep/tf12_keras_regression-wide_deep-multi-input-output.ipynb">github链接</a>）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/25/JywfBIqtvgZiC2X.png" alt="106-9.png"></p>
<p>一个关于 Wide and Deep 的推荐实验：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_34268753/article/details/92123569">https://blog.csdn.net/weixin_34268753/article/details/92123569</a></p>
<h1 id="超参数搜索"><a href="#超参数搜索" class="headerlink" title="超参数搜索"></a>超参数搜索</h1><p>为什么要超参数搜索？手工去试耗费人力。</p>
<p>有哪些超参数？</p>
<ul>
<li>网络结构参数：几层，每层神经元个数，每层激活函数</li>
<li>训练参数: <code>batch_size</code>，学习率，学习率衰减算法</li>
</ul>
<h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><p>下面链接详细讲解了 Batch Size：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34886403/article/details/82558399">https://blog.csdn.net/qq_34886403/article/details/82558399</a></p>
<p>注意：<strong>Batch Size 增大了，要到达相同的准确度，必须要增大 epoch。</strong></p>
<h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p><strong>固定学习率时</strong>，当到达收敛状态时，会在最优值附近一个<strong>较大的区域内摆动</strong>；而当<strong>随着迭代轮次的增加而减小学习率</strong>，会使得在收敛时，在最优值附近一个<strong>更小的区域内摆动</strong>（之所以曲线震荡朝向最优值收敛，是因为在每一个 mini-batch 中都存在噪音）。</p>
<p>学习率衰减算法有很多，这里仅举最简单的两例。</p>
<h3 id="离散下降（discrete-staircase）"><a href="#离散下降（discrete-staircase）" class="headerlink" title="离散下降（discrete staircase）"></a>离散下降（discrete staircase）</h3><p>对于深度学习来说，每 t 轮学习，学习率减半。对于监督学习来说，初始设置一个较大的学习率，然后随着迭代次数的增加，减小学习率。</p>
<h3 id="指数减缓（exponential-decay）"><a href="#指数减缓（exponential-decay）" class="headerlink" title="指数减缓（exponential decay）"></a>指数减缓（exponential decay）</h3><p>对于深度学习来说，学习率按训练轮数增长指数差值递减。例如：</p>
<script type="math/tex; mode=display">
\alpha = 0.95^{\text{epochnum}} \cdot \alpha_0</script><p>又或者公式为：</p>
<script type="math/tex; mode=display">
\alpha = \frac{k}{\sqrt{\text{epochnum}}}</script><p>其中 epochnum 为当前 epoch 的迭代轮数。不过第二种方法会引入另一个超参 k。</p>
<h2 id="网格搜索、随机搜索"><a href="#网格搜索、随机搜索" class="headerlink" title="网格搜索、随机搜索"></a>网格搜索、随机搜索</h2><p>在 <a href="https://dropsong.github.io/posts/b1b54fd.html">机器学习笔记</a> 中已经提过，我们直接实战。</p>
<p>先看一个朴素的例子：<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2/tf13_keras_regression-hp-search.ipynb">github链接</a> 。</p>
<p>能否配合 sklearn 进行参数搜索？</p>
<p><strong>注意</strong>： scikit-learn 在 1.6 版本中对其内部处理 estimator tags (估计器标签) 的 API 进行了现代化更改。这导致一些第三方库（包括早期版本的 XGBoost 和 CausalML）在与 scikit-learn 1.6.x 交互时出现了 <code>AttributeError: &#39;super&#39; object has no attribute &#39;__sklearn_tags__&#39;</code> 类似的问题。</p>
<p>一种不够优雅的解决方案是降级 scikit-learn, <del>我就不尝试了。</del> <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2/tf14_keras_regression-hp-search-sklearn.ipynb">github链接</a> .</p>
<p>另外一些可能的方案有：KerasTuner 等。</p>
<p>关于网格搜索、随机搜索：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/26/CRHAvt9Z1epwhbO.png" alt="106-10.png"></p>
<p>随机搜索：参数的生成方式为随机，<strong>可探索的空间更大</strong>。</p>
<h1 id="tensorflow-基础-API"><a href="#tensorflow-基础-API" class="headerlink" title="tensorflow 基础 API"></a>tensorflow 基础 API</h1><p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/tf%E7%9A%84%E5%9F%BA%E7%A1%80API/tf15-basic-api.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>常量张量及其操作</li>
<li>矩阵乘法</li>
<li>RaggedTensor</li>
<li>SparseTensor</li>
<li>变量张量</li>
<li>reduce_mean</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/tf%E7%9A%84%E5%9F%BA%E7%A1%80API/tf16-keras-regression-customized-loss.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>自定义损失函数</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/tf%E7%9A%84%E5%9F%BA%E7%A1%80API/tf17-keras-regression-customized-layer.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>自定义全连接层<ul>
<li>重写 <code>__init__</code></li>
<li>重写 <code>build</code>，compile 的时候就会调用 build</li>
<li>重写 <code>call</code></li>
</ul>
</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/tf%E7%9A%84%E5%9F%BA%E7%A1%80API/tf18-function-and-auto-graph-1.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li><code>@tf.function</code></li>
<li><code>@tf.py_function</code></li>
<li>把 py 实现的函数变为图实现的函数</li>
<li>找回原来的 py 函数</li>
<li>查看 tf 的图的代码</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/tf%E7%9A%84%E5%9F%BA%E7%A1%80API/tf19-diffs.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>导数、偏导数的近似计算</li>
<li><code>with tf.GradientTape() as tape</code></li>
<li>二阶导数</li>
<li>模拟梯度下降算法</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/tf%E7%9A%84%E5%9F%BA%E7%A1%80API/tf20-keras-regression-manu-diffs.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>手动模拟 fit 操作</li>
<li><code>tf.squeeze()</code></li>
</ul>
<h1 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h1><p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/dataset%E7%9A%84%E4%BD%BF%E7%94%A8/tf21-data-basic-api.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>dataset 的基本常识</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/dataset%E7%9A%84%E4%BD%BF%E7%94%A8/tf22-data-generate-csv.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>在 kaggle 平台上操作数据</li>
<li>数据存为csv文件</li>
<li><code>tf.data.Dataset.list_files</code></li>
<li><code>tf.io.decode_csv</code></li>
<li><code>.shuffle()</code></li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/dataset%E7%9A%84%E4%BD%BF%E7%94%A8/tf23-tfrecord-basic-api.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>为什么使用 TFRecord</li>
<li><code>tf.train.BytesList</code>, <code>tf.train.FloatList</code>, <code>tf.train.Int64List</code></li>
<li><code>tf.train.Features</code>, <code>tf.train.Example</code></li>
<li>读取 record 并打印</li>
<li>把 tfrecord 存为压缩文件</li>
</ul>
<p>数据保存为 tfrecord, 然后再读出来训练： <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/dataset%E7%9A%84%E4%BD%BF%E7%94%A8/tf24_data_generate_tfrecord.ipynb">notebook</a></p>
<h1 id="estimator"><a href="#estimator" class="headerlink" title="estimator"></a>estimator</h1><p>Estimator API 是 TensorFlow 1.x 时代的主要训练方式，现在为了简化和统一，官方推荐直接使用 Keras 的 <code>model.fit()</code>, <code>model.evaluate()</code>, <code>model.predict()</code> .</p>
<p>即便如此，我保留了一些有价值的代码片段。</p>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/estimator/tf25-keras-to-estimator.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>使用 titanic 数据集</li>
<li><strong>将特征预处理作为模型的一部分</strong>，使得模型部署和迁移都更加方便<ul>
<li><code>StringLookup</code>, <code>CategoryEncoding</code></li>
</ul>
</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/estimator/tf26_premade_estimators.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>旧范式，仅参考</li>
</ul>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>什么是卷积神经网络？</p>
<ul>
<li><strong>(卷积层+(可选)池化层)×N+全连接层×M</strong> ,N&gt;=1, M&gt;=0</li>
<li>卷积层的输入和输出都是矩阵，全连接层的输入和输出都是向量</li>
</ul>
<h2 id="全连接层的困境"><a href="#全连接层的困境" class="headerlink" title="全连接层的困境"></a>全连接层的困境</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/17/O5XT7A3HYPnrMEG.png" alt="106-11.png"></p>
<p>上图中，图像大小 1000×1000， 一层神经元数目为 10^6（<strong>经验：神经元数目应与特征数的数量差不多，这样效果好</strong>），这样全连接参数为 <code>1000*1000*10^6 = 10^12</code>，一层就是 1 万亿个参数，内存装不下。</p>
<p>除此之外，全连接层还有参数过多容易过拟合的问题：</p>
<ul>
<li>计算资源不足</li>
<li>容易过拟合，发生过拟合，我们就需要更多训练数据，但是很多时候我们没有更多的数据，因为获取数据需要成本</li>
</ul>
<h2 id="卷积的思路"><a href="#卷积的思路" class="headerlink" title="卷积的思路"></a>卷积的思路</h2><h3 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/17/LS4soTJYgqnfKaQ.png" alt="106-12.png"></p>
<p><strong>局部连接</strong>举例：<br>图像大小 1000×1000，下一层神经元为 10^6，局部连接范围为 10×10，也就是只跟图像中的 100 个像素做连接，全连接参数为 <code>10*10*10^6 = 10^8</code> .</p>
<p>上图反而不太好理解？我这里画一个个人的理解（仅粗糙的原理示意）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/20/xFJ9WrQnEoXHu1V.png" alt="106-13png.png"></p>
<p><strong>为什么可以这么做？</strong><br>局部连接：图像的区域性，爱因斯坦的嘴唇附近的色彩是相似的。</p>
<p><code>10^8</code> 依然是很多的！</p>
<h3 id="卷积操作、参数共享"><a href="#卷积操作、参数共享" class="headerlink" title="卷积操作、参数共享"></a>卷积操作、参数共享</h3><p>如何做卷积？把卷积核和它相对应部分做乘法，然后再求和。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/20/Dk8mdFaIQsHC56M.jpg" alt="106-14.jpeg"></p>
<p>这时我们得到启发，我们将一张图片通过某种映射关系压缩了（尽管这种压缩对于人眼来说可能是无意义的）。我们得到减少训练参数的思路。</p>
<p>input: 一张图片，output: 经过卷积操作后缩小的图片。<br>我们只需要调整卷积核里面的参数，这里面的参数是需要学习的。</p>
<p><strong>为什么可以这么做？</strong><br><strong>参数共享</strong>与<strong>平移不变性</strong>：图像特征与位置无关。左边是脸，右边也是脸，这样无论脸放在什么地方都检查出来，刚好可以解决过拟合的问题（否则脸放到其他地方就检测不出来）。<br>更多、更详细的参考：<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%9A%84%E7%94%B1%E6%9D%A5%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8%E5%8D%B7%E7%A7%AF%EF%BC%9F_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E5%8D%B7%E7%A7%AF-CSDN%E5%8D%9A%E5%AE%A2.pdf">https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%9A%84%E7%94%B1%E6%9D%A5%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8%E5%8D%B7%E7%A7%AF%EF%BC%9F_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E5%8D%B7%E7%A7%AF-CSDN%E5%8D%9A%E5%AE%A2.pdf</a></p>
<p>考虑卷积操作的步长：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/20/r5spfoED98ORThW.png" alt="106-15.png"></p>
<p><code>输出图像尺寸 = 1+(n-k)//s</code>, s 是步长。</p>
<p>新的问题：步长变大，输出变小，那最后导致图像没了怎么办？或者在一些边界上怎么处理？<br>采用如下 <strong>padding</strong> 手法：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/20/WCm5i1XZeSDvK7E.png" alt="106-16.png"></p>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。本质是<strong>降采样</strong>，可以大幅减少网络的参数量。</p>
<p>池化技术的本质：在尽可能保留图片空间信息的前提下，降低图片尺寸，提取高层特征，同时减少网络参数量，预防过拟合。图片的主体内容丢失不多，<strong>依然具有平移，旋转，尺度的不变性</strong>，简单来说就是图片的主体内容依旧保存着原来大部分的空间信息。</p>
<p><strong>最大值池化</strong>，能够抑制网络参数误差造成的估计均值偏移的现象。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/21/PDqIvpbW745tsmH.png" alt="106-17.png"></p>
<p><strong>平均值池化</strong>，主要用来抑制邻域值之间差别过大，造成的方差过大。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/21/wAkRG4NBgqDUeIH.png" alt="106-18.png"></p>
<p><strong>池化操作特点</strong>：</p>
<ul>
<li>常使用不重叠、不补零的方式</li>
<li>没有用于求导的参数（没有需要训练的参数，参数个数为 0）</li>
<li>池化层的<strong>超参数</strong>为步长和池化核大小</li>
<li>减少图像尺寸，从而减少计算量</li>
<li>一定程度<strong>平移鲁棒</strong><ul>
<li>比如一只猫移动了一个像素的另外一张图片，我们先做池化，再做卷积，那么最终还是可以识别这个猫</li>
</ul>
</li>
<li>损失了空间位置精度</li>
</ul>
<h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tf27-keras-classification-model-cnn.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li><code>fashion_mnist</code> 数据集上使用卷积神经网络</li>
<li>多通道的图片如何卷积</li>
<li>使用多个卷积核</li>
<li>搭建卷积神经网络的经典流程</li>
<li>卷积神经网络的参数量计算</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tf28-keras-classification-model-cnn-selu.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>在上一个的基础上改变了激活函数</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tf29-keras-classification-model-separable-cnn.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>深度可分离卷积</li>
<li>深度可分离卷积的参数量计算、优化比例</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tf30_10_monkeys_model_1.ipynb">notebook</a> 、<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tf31-10-monkeys-model-1.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>10-monkeys 数据集</li>
<li>图片增强</li>
<li>卷积神经网络进行图像分类</li>
</ul>
<h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>迁移学习在研究领域上属于机器学习。它专注于存储已有问题的解决模型，并将其利用在其他不同但相关的问题上。比如说，<strong>用来辨识汽车的知识（或者是模型）也可以被用来提升识别卡车的能力</strong>。计算机领域的迁移学习和心理学常常提到的学习迁移在概念上有一定关系，但是两个领域在学术上的关系非常有限。</p>
<p>迁移学习的领域很大，它不是深度学习的一个子领域，而是与深度学习<strong>有交叉</strong>的领域。一般来说，迁移学习包括<strong>基于样本的迁移</strong>，<strong>基于特征的迁移</strong>，<strong>基于模型的迁移</strong>，以及<strong>基于关系的迁移</strong>。</p>
<p>相关讨论：<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/41979241">https://www.zhihu.com/question/41979241</a></p>
<blockquote>
<p>迁移学习按照学习方式可以分为基于样本的迁移，基于特征的迁移，基于模型的迁移，以及基于关系的迁移。基于样本的迁移通过对源域中有标定样本的加权利用完成知识迁移；基于特征的迁移通过将源域和目标域映射到相同的空间（或者将其中之一映射到另一个的空间中）并最小化源域和目标域的距离来完成知识迁移；基于模型的迁移将源域和目标域的模型与样本结合起来调整模型的参数；基于关系的迁移则通过在源域中学习概念之间的关系，然后将其类比到目标域中，完成知识的迁移。</p>
</blockquote>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">ResNet</a> 是 2015 年何凯明、张翔宇、任少卿、孙剑共同提出的。</p>
<p>使用更深层的网络时，会发生<strong>梯度消失、梯度爆炸</strong>的问题，这个问题可以通过标准的初始化和正则化层基本解决，这样可以确保几十层的网络仍能够收敛。但是<strong>随着网络层数的增加，梯度消失或者爆炸的问题依旧存在</strong>。</p>
<p>还有一个问题就是<strong>网络的退化</strong>，举个例子，假设已经有了一个最优化的网络结构，是 18 层。当我们设计网络结构的时候，我们并不知道具体多少层的网络是最优结构，假设我们设计了 34 层网络结构，那么多出来的 16 层其实是冗余的，<strong>我们希望训练网络的过程中，模型能够自己训练它们为恒等映射</strong>，也就是经过这层时的输入与输出完全一样。但是模型往往很难正确地将这 16 层学习为恒等映射，那么最终得到的效果就会比最优的 18 层网络结构差。这就是网络深度增加时，模型产生退化的现象。它不是由过拟合产生的，而是由冗余的网络层学习了不是恒等映射的参数造成的。</p>
<p>具体哪些层是恒等层，这个由网络训练时自己判断。将原网络的几层改成一个残差块，残差块的具体构造如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/06/24/AnqrZH8smWJEvfU.png" alt="106-19.png"></p>
<p>在第二层输出值激活前加入 X，这条路径称作 shortcut 连接。</p>
<p>我们发现，假设该层是冗余的，在引入 ResNet 之前，我们想让该层学习到的参数能够满足 h(x)=x，即输入是 x，经过该冗余层后，输出仍然为 x. 但是要想学习 h(x)=x 恒等映射时的这层参数比较困难。ResNet 想到避免去学习该层恒等映射的参数，使用了如上图的结构，让 h(x)=F(x)+x; 这里的 F(x) 称作残差项，我们发现，要想让该冗余层能够恒等映射，只需要学习 F(x)=0. 学习 F(x)=0 比学习 h(x)=x 要简单，因为一般每层网络中的参数初始化偏向于 0，这样在相比于更新该网络层的参数来学习 h(x)=x，该冗余层学习 F(x)=0 的更新参数能够更快收敛。</p>
<h2 id="实战-1"><a href="#实战-1" class="headerlink" title="实战"></a>实战</h2><p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tf32-10-monkeys-model-2-resnet50-finetune.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>使用 10-monkeys 数据集</li>
<li>微调 resnet，做图像分类</li>
</ul>
<h1 id="卷积神经网络进阶"><a href="#卷积神经网络进阶" class="headerlink" title="卷积神经网络进阶"></a>卷积神经网络进阶</h1><p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tf33-cifar10-model-1.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li><strong>在 kaggle 上打比赛的大致流程</strong></li>
<li>CIFAR-10 - Object Recognition in Images</li>
<li>在 kaggle 上进行更复杂的输入输出文件操作</li>
<li>使用卷积神经网络</li>
</ul>
<p>模型演变：AlexNet、VGG、ResNet、Inception、MobileNet.</p>
<p>为什么要讲不同的网络结构？</p>
<ul>
<li>不同的网络结构解决的问题不同</li>
<li>不同的网络结构使用的技巧不同</li>
<li>不同的网络结构应用的场景不同</li>
</ul>
<p>模型的进化：</p>
<ul>
<li>更深更宽： AlexNet 到 VGGNet</li>
<li>不同的模型结构： VGG 到 InceptionNet/ResNet</li>
<li>优势组合： Inception+Res = InceptionResNet</li>
<li>自我学习： NASNet</li>
<li>实用： MobileNet</li>
</ul>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>请参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-AlexNet%20-%20%E7%9F%A5%E4%B9%8E.pdf">https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-AlexNet%20-%20%E7%9F%A5%E4%B9%8E.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://learnopencv.com/understanding-alexnet/">https://learnopencv.com/understanding-alexnet/</a></li>
</ul>
<h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p>ImageNet 2014，分类第二，物体检测第一。论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a></p>
<ul>
<li>网络结构层次更深</li>
<li><strong>多使用 3x3 的卷积核</strong><ul>
<li>2 个 3x3 的卷积层可以看做一层 5x5 的卷积层，见图 106-20</li>
<li>3 个 3x3 的卷积层可以看做一层 7x7 的卷积层</li>
<li>1x1 的卷积层可以看做是非线性变换</li>
</ul>
</li>
<li>每经过一个 pooling 层，通道数目翻倍</li>
</ul>
<p>视野域：2 个 3x3 = 1 个 5x5，但是为什么要这样做呢？</p>
<ul>
<li><strong>2 层比 1 层多一次非线性变换</strong></li>
<li><strong>参数降低</strong> 28%， <code>5*5-3*3-3*3=7</code>, <code>7/25=28%</code>，参数量降低 28% .</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/06/lvtDWho6EcGqU2Q.png" alt="106-20.png"></p>
<p>训练技巧：</p>
<ul>
<li>LRN 是局部做归一化</li>
<li>为什么都在后面加层？因为前面经过 maxpooling 后，后面参数没那么多，加层的计算量没那么大</li>
<li>先训练浅层网络如 A，再去训练深层网络多尺度输入</li>
<li>不同的尺度训练多个分类器，然后做 ensemble 随机使用不同的尺度缩放然后输入进分类器进行训练</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/06/bEKz1UZVGqrumIT.png" alt="106-21.png"></p>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/tf34-vgg.ipynb">notebook</a> 包含以下内容：</p>
<ul>
<li>tensorflow 1 的一些使用（过时）</li>
<li>模仿 vgg 的实现</li>
</ul>
<h2 id="ResNet-1"><a href="#ResNet-1" class="headerlink" title="ResNet"></a>ResNet</h2><p>可以参考前面的部分，有这个小标题是为了展示卷积神经网络的发展历史，这里仅放一个 ipynb 文件。</p>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/tf35-resnet.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>使用过时的 tensorflow 1 版本</li>
<li>实现 ResNet 的大致思路</li>
</ul>
<h2 id="InceptionNet"><a href="#InceptionNet" class="headerlink" title="InceptionNet"></a>InceptionNet</h2><p>GoogLeNet/<strong>Inception v1</strong> 架构论文： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</a></p>
<p>InceptionNet 可以看作是一种工程优化，同样的参数量更有效率。</p>
<p>分组卷积：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/07/JThgdmNb81AiHaF.png" alt="106-22.png"></p>
<p>可以参考的内容（机翻注意⚠️）：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/45189981">https://zhuanlan.zhihu.com/p/45189981</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/InceptionNet%20%E4%BB%8Ev1%E5%88%B0v4%E7%9A%84%E6%BC%94%E5%8F%98%20-%20%E7%9F%A5%E4%B9%8E.pdf">https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/InceptionNet%20%E4%BB%8Ev1%E5%88%B0v4%E7%9A%84%E6%BC%94%E5%8F%98%20-%20%E7%9F%A5%E4%B9%8E.pdf</a> （这是备份）</li>
</ul>
<p><strong>InceptionNet v2</strong> 和 <strong>InceptionNet v3</strong> 在同一篇论文中提出：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.00567v3">https://arxiv.org/abs/1512.00567v3</a></p>
<p>这个版本提出了许多升级，这些升级提高了准确性并降低了计算复杂度。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/07/LedRZjXSWsC42zT.png" alt="106-23.png"></p>
<p>3x3 不是最小卷积核，3x3 效果类似于 1x3 加上 3x1，这样比较极致，减少 33% .</p>
<p><strong>Inception-v4</strong>, 论文地址：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07261">https://arxiv.org/abs/1602.07261</a></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/07/z4OYT2GD8nLmgHs.png" alt="106-24.png" title="这里类似于残差连接"></p>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/tf36-inception_net.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>使用过时的 tensorflow 1</li>
<li>实现 inception net 的大致思路</li>
</ul>
<h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><p>MobileNet is a family of convolutional neural network (CNN) architectures designed for efficient image classification, object detection, and other computer vision tasks on mobile and embedded devices. They are known for their small size, low latency, and low power consumption, making them suitable for resource-constrained environments. </p>
<p>论文地址： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></p>
<p>参考文章：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70703846">https://zhuanlan.zhihu.com/p/70703846</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%9C%E5%B7%A1%E7%A4%BC%E2%80%9D%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20MobileNet%EF%BC%8C%E4%BB%8EV1%E5%88%B0V3%20-%20%E7%9F%A5%E4%B9%8E.pdf">https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%9C%E5%B7%A1%E7%A4%BC%E2%80%9D%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20MobileNet%EF%BC%8C%E4%BB%8EV1%E5%88%B0V3%20-%20%E7%9F%A5%E4%B9%8E.pdf</a> （备份）</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/tf37-mobile_net.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>使用过时的 tensorflow 1 代码</li>
<li>自行编写深度可分离卷积</li>
<li>实现 MobileNet 的大致思路</li>
</ul>
<h2 id="不同模型结构的对比"><a href="#不同模型结构的对比" class="headerlink" title="不同模型结构的对比"></a>不同模型结构的对比</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/08/lZyh3xIR1pPDbgL.png" alt="106-25.png"></p>
<p>此外，还有 NASNet，是强化学习，并不在图中。</p>
<p>效果计算量分析（性价比）。横轴是计算量，纵轴是精确率，圆圈大小是模型参数量的多少，可以看到 V3 非常的不错：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/08/7YqxWjKTor1w8JZ.png" alt="106-26.png"></p>
<h1 id="卷积神经网络调参"><a href="#卷积神经网络调参" class="headerlink" title="卷积神经网络调参"></a>卷积神经网络调参</h1><h2 id="不同的优化器"><a href="#不同的优化器" class="headerlink" title="不同的优化器"></a>不同的优化器</h2><p>对于稀疏数据，使用学习率可自适应方法。</p>
<p>SGD 通常训练时间更长，最终效果比较好，但需要好的初始化和 learning rate。</p>
<p><strong>需要训练较深较复杂的网络且需要快速收敛时，推荐使用 adam.</strong> 设定一个比较小的 learning rate 值。</p>
<p>Adagrad, RMSprop, Adam 是比较相近的算法，在相似的情况下表现差不多。</p>
<h3 id="前情提要-梯度下降"><a href="#前情提要-梯度下降" class="headerlink" title="前情提要 - 梯度下降"></a>前情提要 - 梯度下降</h3><p>随机梯度下降的问题：</p>
<ul>
<li>局部极值</li>
<li>Saddle point 问题（鞍点问题）</li>
</ul>
<p><strong>动量（momentum）梯度下降</strong>，通过一个比喻来理解：</p>
<blockquote>
<p>想象一个从山坡上滚下来的小球。在普通的梯度下降中，小球的每一步只取决于当前位置的坡度。而带有动量的小球，则会积累滚动的速度。当它冲下陡坡时，会越滚越快；即使遇到比较平缓的区域，由于“惯性”的存在，它也会继续前行一段距离，而不是立即停下。</p>
</blockquote>
<p>在标准的梯度下降中，参数 $w$ 的更新仅仅依赖于当前位置的梯度 $\nabla J(w)$ 和学习率 $\eta$。更新的向量可以表示为 $\Delta w_t$：</p>
<script type="math/tex; mode=display">
\Delta w_t = -\eta \cdot \nabla J(w_t)</script><p>参数更新规则为：</p>
<script type="math/tex; mode=display">
w_{t+1} = w_t + \Delta w_t = w_t - \eta \cdot \nabla J(w_t)</script><p>动量法引入了一个“速度”向量 $v$ (velocity)，它会累积历史的梯度信息。参数更新分为两步：</p>
<p>首先，计算当前的速度 $v_t$。它由<strong>上一时刻的速度 $v_{t-1}$</strong> (按动量因子 $\gamma$ 衰减) 和 <strong>当前梯度</strong> 共同决定：</p>
<script type="math/tex; mode=display">
v_t = \gamma \cdot v_{t-1} + \eta \cdot \nabla J(w_t)</script><p>然后，使用这个速度 $v_t$ 来更新参数：</p>
<script type="math/tex; mode=display">
w_{t+1} = w_t - v_t</script><ul>
<li>$\gamma$ 是<strong>动量因子 (momentum)</strong>，一个介于 <code>[0, 1]</code> 之间的超参数，通常取值为 0.9 左右。它决定了历史速度的保留程度。</li>
<li>当梯度方向连续保持一致时，速度 $v_t$ 会持续累加，从而在那个方向上<strong>加速</strong>前进。</li>
<li>当梯度方向发生振荡时，历史速度可以抵消一部分相反的梯度，从而对更新路径起到平滑作用，实现<strong>减速</strong>和调整方向。</li>
</ul>
<p>动量法通过累积历史梯度，带来了两个显著的优势：</p>
<ol>
<li><strong>加速收敛 (Accelerated Convergence)</strong><ul>
<li>在损失曲面中平坦但有持续梯度的方向（好比一个长长的缓坡峡谷），动量项会累积梯度，使得参数更新的步长越来越大，从而更快地穿越这些区域，整体上加速收敛。</li>
</ul>
</li>
<li><strong>提高精度与稳定性 (Improved Precision and Stability)</strong><ul>
<li>该优势主要体现在<strong>减少振荡</strong>。在损失曲面陡峭但梯度方向来回摆动的方向（好比峡谷的两壁），当前梯度与历史速度方向可能相反。动量项可以起到平滑和抑制作用，有效衰减这种振荡，使得更新方向更稳定地指向最优解。</li>
</ul>
</li>
</ol>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>调整学习率，学习率随着训练次数的增加越来越小。</p>
<script type="math/tex; mode=display">
n_t = n_{t-1} + g_t^2</script><p>以往梯度的平方和作为分母，每次计算要除以它：</p>
<script type="math/tex; mode=display">
\Delta \theta_t = - \frac{\eta}{\sqrt{n_t+ \epsilon}} \cdot g_t</script><p>伪代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  grad_squared += dx * dx</span><br><span class="line">  x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>前期，regularizer 较小，放大梯度；后期，regularizer（分母）较大，缩小梯度。梯度随训练次数降低。</p>
<p>AdaGrad 算法<strong>缺点</strong>：</p>
<ul>
<li>学习率设置太大，导致 regularizer（分母）影响过于敏感</li>
<li>后期，regularizer 累积值太大，提前结束训练</li>
</ul>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>Adagrad 的变种，作了一些调整，由累积平方梯度变为平均平方梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-decay_rate) * dx * dx</span><br><span class="line">  x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>解决了后期提前结束的问题。</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>伪代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">first_moment = <span class="number">0</span></span><br><span class="line">second_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  first_moment = beta1 * first_moment + (<span class="number">1</span>-beta1) * dx  <span class="comment"># Momentum</span></span><br><span class="line">  second_moment = beta2 * second_moment + (<span class="number">1</span>-beta2) * dx * dx  <span class="comment"># Adagrad</span></span><br><span class="line">  x -= learning_rate * first_moment / (np.sqrt(second_moment)+<span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>又加入了一个校准，校准的目的是一开始时，让 <code>first_moment</code> 和 <code>second_moment</code> 都变的稍微大一些：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">first_moment = <span class="number">0</span></span><br><span class="line">second_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_iterations):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  first_moment = beta1 * first_moment + (<span class="number">1</span>-beta1) * dx</span><br><span class="line">  second_moment = beta2 * second_moment + (<span class="number">1</span>-beta2) * dx * dx</span><br><span class="line">  first_unbias = first_moment / (<span class="number">1</span>-beta1**t)</span><br><span class="line">  second_unbias = second_moment / (<span class="number">1</span>-beta2**t)</span><br><span class="line">  x -= learning_rate * first_unbias / (np.sqrt(second_unbias)+<span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>参考之前的内容。</p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><ul>
<li>输入非常大或非常小时没有梯度</li>
<li>输出均值是 0: 相对于 sigmoid 的好处</li>
<li>计算复杂</li>
</ul>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>优点：</p>
<ol>
<li>使用 ReLU 的 SGD 算法的收敛速度比 sigmoid 和 tanh 快。</li>
<li>在 x&gt;0 区域上，不会出现梯度饱和、梯度消失的问题。</li>
<li>计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。</li>
</ol>
<p>缺点：</p>
<ol>
<li>ReLU 的输出不是 0 均值的。</li>
<li><strong>Dead ReLU Problem(神经元坏死现象)</strong>：ReLU 在负数区域被 kill 的现象叫做 dead relu. ReLU 在训练的时很“脆弱”。在 x&lt;0 时，梯度为 0. 这个神经元及之后的神经元梯度永远为 0，不再对任何数据有所响应，导致相应参数永远不会被更新。</li>
</ol>
<blockquote>
<p>一个非常大的梯度流过神经元，不会再对数据有激活现象了，“很大的梯度流过神经元”的意思就是指：该神经元相关的参数被梯度下降算法更新了一次。<br>比如原来的参数可能是：[-10, 5, 7], 然后突然来了一个梯度是[-100, -100, -100], 这样参数就更新成了 [-110, -95, -93]。 然后如果接下来的收到的数据都是[a, b, c], 其中 a, b, c &gt;=0, 这个时候神经元的输出恒为 0，于是不会再有梯度传回来。因而参数得不到更新，也就变成了 dead cell 了。</p>
</blockquote>
<p>产生这种现象的原因：</p>
<ol>
<li>参数初始化问题</li>
<li>learning rate 太高导致在训练过程中参数更新太大</li>
</ol>
<p>解决方法：<br>采用 Xavier 初始化方法，以及避免将 learning rate 设置太大或使用 adagrad 等自动调节 learning rate 的算法。</p>
<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky-ReLU"></a>Leaky-ReLU</h3><p>Leaky ReLU is <strong>an activation function in neural networks that addresses the “dying ReLU” problem by allowing a small, non-zero gradient for negative inputs</strong>, unlike the standard ReLU which outputs zero. This helps prevent neurons from becoming completely inactive during training, especially in deep networks. </p>
<script type="math/tex; mode=display">
\text{LeakyReLU}(x) =
\begin{cases} 
x,  & x>0 \\
\alpha x, & x \le 0
\end{cases}</script><p>其中，$ \alpha $ 通常取值 0.01 ，Leaky-ReLU 的函数范围是 $(-\infty, +\infty)$ .</p>
<h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><p>具有 relu 的优势，没有 Dead ReLU 问题，输出均值接近 0，实际上 PReLU 和 Leaky ReLU 都有这一优点。<strong>有负数饱和区域，从而对噪声有一些鲁棒性。</strong> 可以看做是介于 ReLU 和 LeakyReLU 之间的一个函数。当然，这个函数小于零时也需要计算 exp，从而计算量更大。</p>
<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><script type="math/tex; mode=display">
\text{max}(w_1^T x + b_1, w_2^T x + b_2)</script><p>没有 dead relu 问题。参数翻倍。</p>
<p>优点：</p>
<ol>
<li>Maxout 的拟合能力非常强，可以拟合任意的凸函数</li>
<li>Maxout 具有 ReLU 的所有优点，线性、不饱和性</li>
<li>不会出现神经元坏死的现象</li>
</ol>
<p>缺点：<br>增加了太多参数量。maxout 还需要反向传播去更新它自身的权重系数。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Relu, 小心设置 learning rate（初始值不要太大）</li>
<li>不要使用 sigmoid（relu 出现后，就不用它了，收敛太慢）</li>
<li>使用 LeakyRelu、maxout、ELU</li>
<li>可以试试 tanh，但不要抱太大期望（因为计算量大）</li>
</ul>
<h2 id="网络初始化（w和b）"><a href="#网络初始化（w和b）" class="headerlink" title="网络初始化（w和b）"></a>网络初始化（w和b）</h2><p>有关论文： <a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a> .</p>
<p>论文解析： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/43840797">https://zhuanlan.zhihu.com/p/43840797</a></p>
<p><strong>如何分析初始化结果好不好？</strong></p>
<p>查看初始化后各层的激活值分布，激活值就是经过激活函数后的值，如果分布是固定的在 -1到 1，或者 0 到 1 之间，那没问题，如果集中在某一个值，那就不太好。</p>
<p>用均值为 0，方差为 0.02 的<strong>正态分布</strong>初始化（tanh），如下图所示（依次往后看是经过每一层后的效果），高层均值为 0，<strong>没有梯度</strong>：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/10/DRX4E9VPfO5aIzZ.png" alt="106-27.png"></p>
<p>用均值为 0，方差为 1 的<strong>正态分布</strong>初始化（tanh），如下图所示，高层均值为-1,1，已经饱和：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/10/3VTSKDWIJPLzGiR.png" alt="106-28.png"></p>
<p><strong>Xavier 分布（glorot_uniform）</strong> 在 tanh 上表现很好：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/10/yEM6junIYVicbKN.png" alt="106-29.png"></p>
<p><strong>Xavier 分布在 ReLU 上表现并不好：</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/10/vJKjpZIVHqBsf6W.png" alt="106-30.png"></p>
<p>何凯明发明了 <strong>He Initialization</strong>，对 ReLU 效果较好：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/10/86fQRxE4mCirHSy.png" alt="106-31.png"></p>
<p>关于 He Initialization，可参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://medium.com/@piyushkashyap045/mastering-weight-initialization-in-neural-networks-a-beginners-guide-6066403140e9">Mastering Weight Initialization in Neural Networks: A Beginner’s Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/nn.init.html">PyTorch documentation - torch.nn.init</a> ，网页搜索 kaiming</li>
</ul>
<h2 id="批归一化-1"><a href="#批归一化-1" class="headerlink" title="批归一化"></a>批归一化</h2><p>参照之前。</p>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/10/RtzNQTsCPjhxA7o.png" alt="106-32.png"></p>
<h2 id="更多调参技巧"><a href="#更多调参技巧" class="headerlink" title="更多调参技巧"></a>更多调参技巧</h2><ul>
<li>拿到更多的数据（开发埋点收集更多数据，购买，爬取）</li>
<li>给神经网络添加层次</li>
<li>紧跟最新进展，使用新方法（看论文）</li>
<li>增大训练的迭代次数</li>
<li>尝试正则化 $||w||^2$</li>
<li>使用更多的 GPU 来加速训练 （利用集群）</li>
<li><strong>可视化工具来检查中间状态</strong></li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/10/BdPuv2eGbZksNCL.png" alt="106-33.png"></p>
<p>在预训练好的网络结构上微调（Fine-tuning），例如各种 application. 这种方法在比赛中被大量使用。</p>
<h2 id="实战-2"><a href="#实战-2" class="headerlink" title="实战"></a>实战</h2><p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82/tf38-vgg-tensorboard.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>使用 tensorflow 1 的 tensorboard</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82/tf39-vgg-tensorboard-fine-tune.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>使用老旧的 tensorflow 1</li>
<li>加载、保存模型的方法</li>
<li>checkpoint</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82/tf40-vgg-tensorboard-activation-initializer-optimizer.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>老旧的 tensorflow 1 版本</li>
<li>尝试调整激活函数、初始化分布、优化器</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82/tf41-vgg-tensorboard-data_aug.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>老旧的 tensorflow 1 版本</li>
<li>图像增强</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82/tf42-vgg-tensorboard-data_aug-deeper.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>老旧的 tensorflow 1 版本</li>
<li>更深的网络深度</li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82/tf43-vgg-tensorboard-data_aug-deeper-bn.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>老旧的 tensorflow 1 版本</li>
<li>卷积搞为装饰器（类似），这样方便很多</li>
<li>批归一化</li>
</ul>
<h1 id="图像风格变换"><a href="#图像风格变换" class="headerlink" title="图像风格变换"></a>图像风格变换</h1><h2 id="神经网络可以做什么"><a href="#神经网络可以做什么" class="headerlink" title="神经网络可以做什么"></a>神经网络可以做什么</h2><p>图像风格转换：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgs.ovh/2025/07/12/sxObm.png" alt="106-34"></p>
<p>图像修复（第三列是<strong>传统修复</strong>，第四列是<strong>深度学习修复</strong>）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/12/OkArDNZoCTXn16t.png" alt="106-35.png"></p>
<p>换脸（第二个是传统地 P 了一张脸上去，第三个是<strong>可以保证脸型和肤色还是原有的</strong>）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/12/oRSbjAgJHxc5XrV.png" alt="106-36.png"></p>
<p>还有图片超清化，图像生成文字等。</p>
<h2 id="卷积神经网络学到了什么"><a href="#卷积神经网络学到了什么" class="headerlink" title="卷积神经网络学到了什么"></a>卷积神经网络学到了什么</h2><p>参考论文： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</a></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2025/07/12/68713aa5e62fa.png" alt="106-37.png"></p>
<p>经验总结（一种尝试性质的解释）：</p>
<ul>
<li>卷积神经网络的每一层的激活值都可以看做是图像的抽象表示</li>
<li>卷积神经网络中某层的每个激活值都可以看做是一个分类器，众多的分类结果组成了抽象表示</li>
<li>层级越高，特征抽象程度越高</li>
</ul>
<h2 id="图像风格转换（简单版本）"><a href="#图像风格转换（简单版本）" class="headerlink" title="图像风格转换（简单版本）"></a>图像风格转换（简单版本）</h2><p>参考论文： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a></p>
<p>发展历史： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26746283">图像风格迁移(Neural Style)简史</a>，内容已在 github 备份。</p>
<p>不可能存在一个风格变换的目标值，所以是一个<strong>无监督问题</strong>。</p>
<p>希望的效果：<strong>输出图像既和原图很相近，也和风格图片很相近。</strong></p>
<blockquote>
<p>根据之前的笔记内容 <strong>“卷积神经网络学到了什么”</strong>，对于原图，可以计算其内容特征；对于风格图片，可以计算其风格特征。</p>
</blockquote>
<p>图片在 CNN 的不同处理阶段中表现出不同的 filtered images ：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/21/5nQdy8SrgYNkBqU.png" alt="106-38.png"></p>
<p><strong>重要性质</strong>：</p>
<ol>
<li><strong>Higher layers in the network capture the high-level content</strong> in terms of objects and their  arrangement in the input image <strong>but do not constrain the exact pixel values</strong> of the reconstruction.<ul>
<li>the size and complexity of local image  structures from the input image increases along the hierarchy, a result that <strong>can be explained  by the increasing receptive field sizes and feature complexity</strong>.</li>
</ul>
</li>
<li>In contrast, <strong>reconstructions from the lower layers  simply reproduce the exact pixel values of the original image</strong>.</li>
</ol>
<p>思路：虽然这是一个无监督的问题，但是为了达到“相近”，还是可以计算损失。</p>
<p>我们希望 <code>内容损失+风格损失</code> 是最小的（这里的 <code>+</code> 不一定是数值上简单的求和）。</p>
<p>所以要计算两个损失：</p>
<ol>
<li>对内容图片的损失</li>
<li>对风格图片的损失</li>
</ol>
<p><strong>对于内容</strong>，关键思路：we <strong>perform gradient descent on a white noise image</strong> to find another image that matches the feature responses of the original image.</p>
<p>为方便起见，记：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/21/15r24vME8y3kCiT.png" alt="106-39.png"></p>
<p>因此 layer $l$ 的 responses 可以存储在矩阵 $F^l \in \cal{R}^{\it{N_l \times M_l}}$ 中，where $F^l_{ij}$ is the activation  of the $i^{th}$ filter at position $j$ in layer $l$.</p>
<p>记原图为 $\vec{p}$ ，$P^l$ 是其在 layer $l$ 对应的 feature representation. 记待生成的图像为 $\vec{x}$，$F^l$ 是其在 layer $l$ 对应的 feature representation. 记损失：</p>
<script type="math/tex; mode=display">
\cal{L}_{\it{content}} (\vec{p}, \vec{x}, l) = \frac{1}{2} \sum _{i,j} (\it{F^l_{ij}} - P^l_{ij})^2</script><p>激活函数是 relu, 则</p>
<script type="math/tex; mode=display">
\frac{\partial \cal{L}_{\it{content}}}{\partial F^l_{ij}} = 
\begin{cases} 
(F^l-P^l)_{ij}  & \text{if }F^l_{ij} > 0 \\
0 & \text{if } F^l_{ij} < 0
\end{cases}</script><p><strong>对于风格</strong>，On top of the CNN responses in each layer of the network we built <strong>a style representation that computes the correlations between the different filter responses</strong>, where the expectation is  taken over the spatial extend of the input image.</p>
<p>特征相关性由 Gram 矩阵 $G^l \in \cal{R}^{\it{N_l} \times N_l}$ 给出，$G^l$ 在第 $i$ 行第 $j$ 列的元素 $G^l_{ij}$ 为：</p>
<script type="math/tex; mode=display">
G^l_{ij} = \sum_k F^l_{ik}F^l_{jk}</script><p>$G^l_{ij}$ 是 layer $l$ 里，向量化的 feature map $i$ 和 $j$ 的内积。示意图如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/21/TrYfiI2bZxSHUQu.png" alt="106-40.png"></p>
<p>因为这里的 feature map 是向量化的（类似于 tensorflow 中的展平操作），所以其实也可以有如下写法：</p>
<script type="math/tex; mode=display">
G^l = F^l \cdot (F^l)^T</script><p>这里同时得出之后会用到的重要性质：$G^l$ 是对称阵，也即 <strong>Gram 矩阵是对称阵</strong> 。</p>
<p>记 $\vec{a}$ 为原始图像，$\vec{x}$ 为待生成的图像，$A^l$ 和 $G^l$ 是 layer $l$ 上对应的风格表示。layer $l$ 对总损失的贡献为：</p>
<script type="math/tex; mode=display">
E_l = \frac{1}{4N^2_l M^2_l} \sum_{i,j} (G^l_{ij}-A^l_{ij})^2</script><p>那么总的损失就是：</p>
<script type="math/tex; mode=display">
\cal{L}_{style}(\vec{a}, \vec{x}) = \sum_{l=0}^{\it{L}} w_l \it{E}_l</script><p>其中，$w_l$ 是不同 layer 的权重系数。</p>
<p>那么，可对 $E_l$ 求导（激活函数 relu 之后）。我们要求的是：</p>
<script type="math/tex; mode=display">
\frac{\partial E_l}{\partial F^l_{pq}}</script><p>这个相当于对 activation map 中的某一个像素值求导。其中，$p$ 表示第 $p$ 个特征图（滤波器），$q$ 表示该特征图中的第 $q$ 个空间位置。</p>
<script type="math/tex; mode=display">
E_l = \frac{1}{4N^2_l M^2_l} \sum_{i,j} \left( \sum_k F^l_{ik}F^l_{jk} - A^l_{ij} \right) ^2</script><p>对 $F^l_{pq}$ 求导数时，只考虑那些包含 $F^l_{pq}$ 的项。观察 Gram 矩阵的定义，我们发现 $F^l_{pq}$ 会出现在所有满足 $i = p$ 或 $j = p$ 的项中。</p>
<p>所以对单个项 $(G^l_{ij} - A^l_{ij})^2$ 的导数如下：</p>
<p><strong>情况 1，当 $i = p$ 时：</strong></p>
<script type="math/tex; mode=display">
\frac{\partial G^l_{ij}}{\partial F^l_{pq}} = \frac{\partial}{\partial F^l_{pq}} \sum_k F^l_{ik} F^l_{jk} = F^l_{jq} \quad (\text{因为 } i=p, \text{而 } k=q)</script><p><strong>情况 2，当 $j = p$ 时：</strong></p>
<p>同理：</p>
<script type="math/tex; mode=display">
\frac{\partial G^l_{ij}}{\partial F^l_{pq}} = F^l_{iq}</script><p>（如果 $i = j = p$，那就变成两项相加）</p>
<p>于是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_l}{\partial F^l_{pq}} &= \frac{1}{4 N_l^2 M_l^2} \sum_j 2 (G^l_{pj} - A^l_{pj}) F^l_{jq} + \sum_i 2 (G^l_{ip} - A^l_{ip}) F^l_{iq} \\
&= \frac{1}{2 N_l^2 M_l^2} \left[ \sum_j (G^l_{pj} - A^l_{pj}) F^l_{jq} + \sum_i (G^l_{ip} - A^l_{ip}) F^l_{iq} \right]  \\
\end{aligned}</script><p>这个结果不够美观，可以使用矩阵乘法的一些小技巧化简一下。</p>
<p>注意到</p>
<script type="math/tex; mode=display">
\sum_j (G^l_{pj} - A^l_{pj}) F^l_{jq}</script><p>是对<strong>第 $p$ 行的 Gram 差值</strong>与<strong>第 $q$ 列的特征图</strong>相乘：</p>
<center>
<svg xmlns="http://www.w3.org/2000/svg" style="background: transparent; background-color: transparent;" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="422px" height="310px" viewBox="-0.5 -0.5 422 310" content="&lt;mxfile&gt;&lt;diagram id=&quot;L-LIzdtOlmAaIUcO0Fmq&quot; name=&quot;Page-1&quot;&gt;7VpNc9owEP01zLQHOrb8ARwJ+eglnU7TmZ6FLYwmskVkEUh/fSVLxrYsEpIaEiA5EPO8XlnvrXdXwj1vkq5vGFzMb2mMSA848brnXfYAGLmu+JTAkwJCJ1BAwnCsILcC7vBfpEFHo0sco7xhyCklHC+aYESzDEW8gUHG6KppNqOkOeoCJqgF3EWQtNE/OOZzhQ7BoMK/I5zMy5HdcKTOpLA01jPJ5zCmqxrkXfW8CaOUq6N0PUFEclfyoq673nJ2c2MMZXyXC4C64BGSpZ6bvi/+VE4WZfFYcia+ZTQT4MWcp0R8c8Vhezx9CyhuUKhHv0E0RZw9CYNVRVygyZjXOCsxhgjk+LFJPNT6JRt3mxF+UizuBDg61Pyh9qMjzQudpoucLlmE9FV1ol5w5JqOOGQJ4i1H4qA27QoqdLBr4p24JgHoSBPT0R41CU9cE+A0qQT+W5+Tlxx1p8ng3DQZdqWJ6ag7TYY7aEKIqNNSi9Ucc3S3gJE8sxKdQlMfmC9U8Z7hNRL+L2aYkAkllBWOvNkMhVEk8Jwzeo9qZ+LBaCrm/IzEj4hxtH5W5FIF1yCvVLcWBMASBMDZrneD4GfYHJ0cm/7ISNmDw7Hp7tDs/C+JwwjZSZwOAz/oKiSD9wtJ1z8REs1I3DznhyAxsJAYEi65oEXmninyBPqwlGuBi984FSse4PxAK/H5i6Ywq0426C9B6amfF2unsTAQdK0L0+qiMNH/i5GnJXBTIvlyUWKkxMTUanB/3LC13EV7vC2uaujUgileStgIOCE5N2KM4CQTx5EIECSi50IGBhYruLE+keI4JtvSJqPLLJahednVE+sHjWDzLE+s61qibdBFtNmaV0Nz2SOJOdpFODa2Xd/sQYMW26GFbK8Lsm1dqUF2RMkyFY+v83AafPsDo6iDdnTvjW9bx6lIjfGjNSVJPvuaQpkZNyzukBs36W4h86/F+RRG90nBcD9ShUsOwZLpFxCIKJzITFw7+Kqyt2E6cJVB9V/bFTl9BlNMnpT1mGFIlFEOs7yfI4ZnNdsq/7u+zv+OOf9ctP/2VP2jyu5Te/q2pWrFzTny1Zt4vdGgO9a21MIisI81WQAjWWwWp4dIFrYF1Yfpu64/e6mdqs179lJlZfssN5/l5t35UuXm9rPcvKI3PWS52dztq3aoY5jPCwbc3nFvVzsGhW/erjYddbddXQpyngp55l7WWxVqOepQIduvoUaV/Fb8ncZa2myPgXO4vQtg29s9J65HB6wNti3gk808vrEj1/qlfufMYzhqvTvQYeZ502/+x6pQEHakkOlonwrtsNf6YfIVo1zoQ+VVo656W8/IX94Ba8X2rZSz4D4wXwvYH/fia/VCoHpMqrcqvat/&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g><g><path d="M 340 240 L 340 40" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 380 240 L 380 40" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 60 120 L 260 120" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 60 160 L 260 160" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><ellipse cx="80" cy="140" rx="10" ry="10" fill="#ffe6cc" stroke="#d79b00" pointer-events="all" style="fill: light-dark(rgb(255, 230, 204), rgb(54, 33, 10)); stroke: light-dark(rgb(215, 155, 0), rgb(153, 101, 0));"/></g><g><ellipse cx="360" cy="60" rx="10" ry="10" fill="#ffe6cc" stroke="#d79b00" pointer-events="all" style="fill: light-dark(rgb(255, 230, 204), rgb(54, 33, 10)); stroke: light-dark(rgb(215, 155, 0), rgb(153, 101, 0));"/></g><g><rect x="110" y="130" width="20" height="20" fill="#f8cecc" stroke="#b85450" pointer-events="all" style="fill: light-dark(rgb(248, 206, 204), rgb(81, 45, 43)); stroke: light-dark(rgb(184, 84, 80), rgb(215, 129, 126));"/></g><g><rect x="350" y="90" width="20" height="20" fill="#f8cecc" stroke="#b85450" pointer-events="all" style="fill: light-dark(rgb(248, 206, 204), rgb(81, 45, 43)); stroke: light-dark(rgb(184, 84, 80), rgb(215, 129, 126));"/></g><g><rect x="105" y="230" width="110" height="70" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 108px; height: 1px; padding-top: 265px; margin-left: 106px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><font style="font-size: 20px;" face="Times New Roman"><b>G<sup>l</sup>-A<sup style="">l</sup></b></font></div></div></div></foreignObject><text x="160" y="269" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Gl-Al</text></switch></g></g><g><rect x="0" y="125" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 140px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>row p</b></div></div></div></foreignObject><text x="30" y="144" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">row p</text></switch></g></g><g><rect x="330" y="0" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 15px; margin-left: 331px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>column q</b></div></div></div></foreignObject><text x="360" y="19" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">column q</text></switch></g></g><g><rect x="130" y="280" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 295px; margin-left: 131px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><div style="text-align: center;"><b><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">N<sub>l</sub></span><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">×N<sub>l</sub></span></b></div></div></div></div></foreignObject><text x="160" y="299" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Nl×Nl</text></switch></g></g><g><rect x="305" y="230" width="110" height="70" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 108px; height: 1px; padding-top: 265px; margin-left: 306px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><font style="font-size: 20px;" face="Times New Roman"><b>F<sup style="">l</sup></b></font></div></div></div></foreignObject><text x="360" y="269" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Fl</text></switch></g></g><g><rect x="330" y="280" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 295px; margin-left: 331px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><div style="text-align: center;"><b><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">N<sub>l</sub></span><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">×M<sub>l</sub></span></b></div></div></div></div></foreignObject><text x="360" y="299" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Nl×Ml</text></switch></g></g><g><path d="M 60 80 L 260 80" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 60 200 L 260 200" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><rect x="130" y="85" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 100px; margin-left: 131px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="160" y="104" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g><g><rect x="130" y="170" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 185px; margin-left: 131px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="160" y="189" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g><g><path d="M 300 40 L 300 240" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 420 40 L 420 240" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><rect x="290" y="115" width="60" height="30" fill="none" stroke="none" transform="rotate(90,320,130)" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)rotate(90 320 130)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 130px; margin-left: 291px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="320" y="134" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g><g><rect x="370" y="115" width="60" height="30" fill="none" stroke="none" transform="rotate(90,400,130)" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)rotate(90 400 130)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 130px; margin-left: 371px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="400" y="134" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.drawio.com/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Text is not SVG - cannot display</text></a></switch></svg>
</center>

<p>类似地，对于</p>
<script type="math/tex; mode=display">
\sum_i (G^l_{ip} - A^l_{ip}) F^l_{iq}</script><p>是对<strong>第 $p$ 列的 Gram 差值</strong>与<strong>第 $q$ 列的特征图</strong>相乘：</p>
<center>
<svg xmlns="http://www.w3.org/2000/svg" style="background: transparent; background-color: transparent;" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="322px" height="310px" viewBox="-0.5 -0.5 322 310" content="&lt;mxfile&gt;&lt;diagram id=&quot;L-LIzdtOlmAaIUcO0Fmq&quot; name=&quot;Page-1&quot;&gt;7VpLj9owEP41SO2BKrGTAEdgH73squpW6tkkJljrxKxjXv31dWKHPHAWqgZYFjhA8mU8jr8Zz9geOnAcrR85ms+eWIBpB1jBugPvOgAMLEd+p8BGAZ7lKiDkJFCQXQAv5A/WoKXRBQlwUhEUjFFB5lXQZ3GMfVHBEOdsVRWbMlrtdY5CvAO8+Ijuor9JIGYK7YNegX/HJJzlPdveQD2JUC6sR5LMUMBWJQjed+CYMybUVbQeY5pyl/Oi2j00PN2+GMexOKQBUA2WiC702PR7iU0+WBwHw5QzeRezWIKjmYiovLPl5W5/+hVwUKFQ9/6IWYQF30iBVUGcq8mYlTjLMY4pEmRZJR5p+4VbddsefjAi3wRY2tWcvtajPQ16VlVFwhbcx7pVmag9iuy6IoF4iMWOInlRGnYBZXYw2wR+cpu4oCWb1BUd0SaDA2xCqYw/qS1WMyLwyxz56ZOVjIBV+6BkroLSlKyx1D+aEkrHjDKeKYLTKfZ8X+KJ4OwVl54EvcHEst4z8RJzgdfvGjl36EGNvJ6+LzkBMDgBsJrtXSH4HTZtZz+d/0ti38dmEid913GPROI2Q52CRNdAokdFygXLpslUkSfRt0WaUEa/SCTTJrCe8Up+/2QRiouHFfpzMNXUTbIEPJQCkq51Jlo08kL9m/U8yYHHHEkW8xyjOSaHVoK7w4qs4S12+2tQVUInBkzxksM1h5MmFzUfoySM5bUvHQRL7xmljkHkMmCoH0QkCGjTjOdsEQepa9615GzAcatx0911Nts2eFuvDW/rNXrblmmf0UUkPcp6M9vh0gh3erUQCXYJ9wx8wzb47jfyHZClcZakfHY1helk3bJ4wHTdzsB5GhIMyifIfw0zhru+iqVpFzycfAGudMtxGhxKF19VQKmJ9mwlUPxquSzMTFFE6EZJDzlBVAklKE66CeZkWpItQpLt6JBk1cefyPRvjh7PRcCZmCOKKXoobq6Rr84Ydga99lhrCM+ZY19qsAC1YOFYJwwWpuXph1kKPNzS+0HZ5pzpPc9st3RzSzdn50ulm6dbuvmHtekp0w0w7Tz3nlAFKJllDNidizqucpw9p0wHHyHWFO2ce7V3XAW8a7KQ67VkobqiY1rogP30t+zzAfbSnAlpH5a2GrQVv2CVarncOV38al4uXwX3rn0+7qFpobk3Ml1QMAJtVZzqio5ZcbI/uVFgWyWnuqJjGgUcYJTLqjmBM9ac4AFl1YuoOdVJPGXNCZoKd01VgPkHyJ5t8H3GKgC8qp0WaGunVVd0xHU8vKqdFmxrp1VXdEwLXflOC5xxpwWbq5hXwT083U5L3hZ/G1TTpPjvJbz/Cw==&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g><g><path d="M 240 240 L 240 40" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 280 240 L 280 40" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><ellipse cx="260" cy="60" rx="10" ry="10" fill="#ffe6cc" stroke="#d79b00" pointer-events="all" style="fill: light-dark(rgb(255, 230, 204), rgb(54, 33, 10)); stroke: light-dark(rgb(215, 155, 0), rgb(153, 101, 0));"/></g><g><rect x="250" y="90" width="20" height="20" fill="#f8cecc" stroke="#b85450" pointer-events="all" style="fill: light-dark(rgb(248, 206, 204), rgb(81, 45, 43)); stroke: light-dark(rgb(184, 84, 80), rgb(215, 129, 126));"/></g><g><rect x="5" y="230" width="110" height="70" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 108px; height: 1px; padding-top: 265px; margin-left: 6px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><font style="font-size: 20px;" face="Times New Roman"><b>G<sup>l</sup>-A<sup style="">l</sup></b></font></div></div></div></foreignObject><text x="60" y="269" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Gl-Al</text></switch></g></g><g><rect x="230" y="0" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 15px; margin-left: 231px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>column q</b></div></div></div></foreignObject><text x="260" y="19" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">column q</text></switch></g></g><g><rect x="30" y="280" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 295px; margin-left: 31px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><div style="text-align: center;"><b><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">N<sub>l</sub></span><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">×N<sub>l</sub></span></b></div></div></div></div></foreignObject><text x="60" y="299" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Nl×Nl</text></switch></g></g><g><rect x="205" y="230" width="110" height="70" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 108px; height: 1px; padding-top: 265px; margin-left: 206px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><font style="font-size: 20px;" face="Times New Roman"><b>F<sup style="">l</sup></b></font></div></div></div></foreignObject><text x="260" y="269" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Fl</text></switch></g></g><g><rect x="230" y="280" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 295px; margin-left: 231px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><div style="text-align: center;"><b><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">N<sub>l</sub></span><span style="background-color: light-dark(rgb(255, 255, 255), rgb(18, 18, 18)); color: light-dark(rgb(71, 71, 71), rgb(176, 176, 176)); font-family: Arial, sans-serif; font-size: 14px; text-align: start;">×M<sub>l</sub></span></b></div></div></div></div></foreignObject><text x="260" y="299" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">Nl×Ml</text></switch></g></g><g><path d="M 200 40 L 200 240" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 320 40 L 320 240" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><rect x="190" y="115" width="60" height="30" fill="none" stroke="none" transform="rotate(90,220,130)" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)rotate(90 220 130)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 130px; margin-left: 191px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="220" y="134" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g><g><rect x="270" y="115" width="60" height="30" fill="none" stroke="none" transform="rotate(90,300,130)" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)rotate(90 300 130)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 130px; margin-left: 271px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="300" y="134" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g><g><path d="M 40 240 L 40 40" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 80 240 L 80 40" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><ellipse cx="60" cy="60" rx="10" ry="10" fill="#ffe6cc" stroke="#d79b00" pointer-events="all" style="fill: light-dark(rgb(255, 230, 204), rgb(54, 33, 10)); stroke: light-dark(rgb(215, 155, 0), rgb(153, 101, 0));"/></g><g><rect x="50" y="90" width="20" height="20" fill="#f8cecc" stroke="#b85450" pointer-events="all" style="fill: light-dark(rgb(248, 206, 204), rgb(81, 45, 43)); stroke: light-dark(rgb(184, 84, 80), rgb(215, 129, 126));"/></g><g><rect x="30" y="0" width="60" height="30" fill="none" stroke="none" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 15px; margin-left: 31px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>column p</b></div></div></div></foreignObject><text x="60" y="19" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">column p</text></switch></g></g><g><path d="M 0 40 L 0 240" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><path d="M 120 40 L 120 240" fill="none" stroke="#000000" stroke-miterlimit="10" stroke-dasharray="3 3" pointer-events="stroke" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));"/></g><g><rect x="-10" y="115" width="60" height="30" fill="none" stroke="none" transform="rotate(90,20,130)" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)rotate(90 20 130)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 130px; margin-left: -9px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="20" y="134" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g><g><rect x="70" y="115" width="60" height="30" fill="none" stroke="none" transform="rotate(90,100,130)" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)rotate(90 100 130)"><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 130px; margin-left: 71px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: &quot;Helvetica&quot;; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><b>......</b></div></div></div></foreignObject><text x="100" y="134" fill="light-dark(#000000, #ffffff)" font-family="&quot;Helvetica&quot;" font-size="12px" text-anchor="middle">......</text></switch></g></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.drawio.com/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Text is not SVG - cannot display</text></a></switch></svg>
</center>

<p><strong>对于上面的两个式子，只需要按照 dot product 的方法算出两个数值即可。</strong> （自行体会）</p>
<p>既然只需要算出数值，为了简化形式，我们可以尝试合并。同时，我们注意到一个很好的性质：</p>
<script type="math/tex; mode=display">
(G^l - A^l) = (G^l - A^l)^T</script><p>且 $(G^l - A^l)$ 是对称阵，形状为 $N_l \times N_l$ .</p>
<p>我们很有注意力地简化为：</p>
<script type="math/tex; mode=display">
\frac{\partial E_l}{\partial F^l_{pq}} = \frac{1}{N^2_l M^2_l} \left( (F^l)^T (G^l - A^l)^T \right)_{qp}</script><p><strong>注意细节</strong>：$p$ 和 $q$ 交换了位置。这是为了统一形式，做了转置之后位置也要对应上。</p>
<p>可以进一步简化为：</p>
<script type="math/tex; mode=display">
\frac{\partial E_l}{\partial F^l_{pq}} = \frac{1}{N^2_l M^2_l} \left( (F^l)^T (G^l - A^l) \right)_{qp}</script><p>实际上还要经过一次 relu, 因为在大于 0 的时候 relu 相当于恒等映射，小于 0 时值变为 0，所以最终的公式是：</p>
<script type="math/tex; mode=display">
\frac{\partial E_l}{\partial F^l_{pq}} =
\begin{cases}
\frac{1}{N_l^2 M_l^2} \left( (F^l)^T (G^l - A^l) \right)_{qp} & F^l_{pq} > 0 \\
0 & F^l_{pq} < 0
\end{cases}</script><p>为了<strong>综合考虑内容损失和风格损失</strong>：</p>
<script type="math/tex; mode=display">
\cal{L}_{total} (\vec{p}, \vec{a}, \vec{x}) = \alpha \cal{L}_{content} (\vec{p}, \vec{x}) + \beta \cal{L}_{style} (\vec{a}, \vec{x})</script><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/21/tVTz59FLJIMmGWX.png" alt="106-41.png"></p>
<p>关于代码实现，请参考 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/tree/master/%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E5%8F%98%E6%8D%A2/style-transfer">此链接</a> 。</p>
<h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p>另一个版本：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/21/FemaD49L8Uiwn3f.png" alt="106-42.png"></p>
<p>另一个版本：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/21/Qwa1ylbJpYkur48.png" alt="106-43.png"></p>
<p>可以用 Google Lens 搜图，查论文。</p>
<h1 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h1><p>为什么要提出 embedding? Embedding 可以干什么？</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35799003/article/details/84780289">https://blog.csdn.net/qq_35799003/article/details/84780289</a></li>
<li>以上链接内容已备份（<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/embedding/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84embedding_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0embedding-CSDN%E5%8D%9A%E5%AE%A2.pdf">github 对应链接</a>）</li>
</ul>
<p>理解 Embedding 原理：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">秒懂词向量Word2vec的本质</a>，<a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/embedding/%E7%A7%92%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8FWord2vec%E7%9A%84%E6%9C%AC%E8%B4%A8%20-%20%E7%9F%A5%E4%B9%8E.pdf">备份</a></li>
</ul>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/embedding/tf44-embedding-padding-pooling.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>数据集imdb，电影分类，分电影是积极的，还是消极的</li>
<li>变长输入</li>
<li><code>keras.preprocessing.sequence.pad_sequences()</code></li>
<li>embedding 理解</li>
<li><code>keras.layers.Embedding()</code></li>
<li><code>keras.layers.GlobalAveragePooling1D()</code></li>
</ul>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>RNN 的基础知识： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30844905">https://zhuanlan.zhihu.com/p/30844905</a>  （已在 github 对应仓库备份）</p>
<p>RNN 的一个经典应用（例如可以用于输入法的下一个词预测）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/25/nZpVlLbXvzQi2DH.png" alt="106-44.png"></p>
<p>RNN 的反向传播：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/25/tE4e8NqOCrQAK1l.jpg" alt="106-45.jpeg"></p>
<p>较远的步骤梯度贡献很小，实践中可以把他们忽略：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/25/tywrH8PTGkMji9O.jpg" alt="106-46.jpeg"></p>
<p>自然，可以有多层的 RNN：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/25/MNtJI2C5uelSm3Y.png" alt="106-47.png"></p>
<p>类似地，也可以在 RNN 上搞<strong>残差连接</strong>，按下不表。</p>
<p>双向网络（<strong>上下文</strong>）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/25/ceSOBfTuAI2MrsX.png" alt="106-48.png"></p>
<p>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/RNN/tf45-embedding-rnn.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>imdb 数据集（完成任务同上一个）</li>
<li><code>keras.layers.SimpleRNN()</code></li>
<li><code>keras.layers.Bidirectional()</code></li>
</ul>
<p>BRNN（双向RNN）的应用包括（ref: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks">Wikipedia</a>）：</p>
<ul>
<li>语音识别（与长时记忆结合）</li>
<li>翻译</li>
<li>手写识别</li>
<li>蛋白质结构预测</li>
<li>词性标记</li>
<li>依赖解析</li>
<li>实体提取</li>
</ul>
<p><strong>文本生成实战</strong><br>本 <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/RNN/tf46-text-generation.ipynb">notebook</a> 包含的内容：</p>
<ul>
<li>莎士比亚数据集</li>
<li>生成文本的原理</li>
<li>模型的保存与加载</li>
<li>该模型的评价</li>
<li><code>Squeeze</code>, <code>expand_dims</code></li>
</ul>
<h1 id="长短期记忆网络-LSTM"><a href="#长短期记忆网络-LSTM" class="headerlink" title="长短期记忆网络 LSTM"></a>长短期记忆网络 LSTM</h1><p><strong>为什么需要 LSTM？</strong><br>普通 RNN 的信息不能长久传播：离结尾较远的信息被稀释的比较厉害。</p>
<p>LSTM 引入<strong>选择性机制</strong>：</p>
<ul>
<li>选择性输出</li>
<li>选择性输入</li>
<li>选择性遗忘</li>
</ul>
<p>选择性机制实现的原理：门。</p>
<ol>
<li>向量 A -&gt; sigmoid -&gt; <code>[0.1, 0.9, 0.4, 0, 0.6]</code></li>
<li>向量 B -&gt; <code>[13.8, 14, -7, -4, 30.0]</code></li>
<li>A 为门限，B 为信息，两个相乘（点积）</li>
<li>A dot B = <code>[1.38, 12.6, -2.8, 0, 18.0]</code></li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/29/3u5lMGo9PfwbDcN.png" alt="106-49.png"></p>
<p>ref: <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44162104/article/details/88660003">https://blog.csdn.net/weixin_44162104/article/details/88660003</a></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/29/tl1WBjK4Q2YoDHR.png" alt="106-50.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/29/V5HrI9W8u2kTsPt.png" alt="106-51.png"></p>
<p>总览：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/29/ndvb1qws4Ih7ykB.png" alt="106-52.png"></p>
<p>从微观上看，LSTM 引入了细胞状态，并使用输入门、遗忘门、输出门来保持和控制信息。具体地，LSTM 某个 timestep $t$ 的计算公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f_t & = \sigma (W_f \cdot [h_{t-1}, x_t] + b_f) \\
i_t & = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i) \\
\tilde{c}_t & = \tanh (W_c \cdot [h_{t-1}, x_t] + b_c) \\
c_t & = f_t * c_{t-1} + i_t * \tilde{c}_t \\
o_t & = \sigma (W_o \cdot [h_{t-1}, x_t] + b_o) \\
\end{aligned}</script><p>其中，$f_t$ 为遗忘门，$i_t$ 为输入门，$o_t$ 为输出门，$c_t$ 为细胞状态，$\tilde{c}_t$ 为细胞状态候选值，$h_t$ 为隐藏层状态值，$W$ 和 $b$ 为权重和偏置。</p>
<p><strong>LSTM 既能够处理短期依赖问题，又能够处理长期依赖问题。</strong></p>
<p>实战： <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/RNN/tf47-embedding-lstm.ipynb">RNN/tf47-embedding-lstm.ipynb</a></p>
<ul>
<li>imdb 数据集</li>
<li>LSTM 文本分类</li>
</ul>
<p>实战： <a target="_blank" rel="noopener" href="https://github.com/dropsong/dl-ipynb-examples/blob/master/RNN/tf48-embedding-lstm-subword.ipynb">RNN/tf48-embedding-lstm-subword.ipynb</a></p>
<ul>
<li>subword<ul>
<li>Subword-level 是介于 char-level 和 word-level 之间的设计，很多机器翻译都会用 subword</li>
<li>词典比 word level 小，embedding 参数变少，训练时间缩短</li>
</ul>
</li>
<li>imdb 数据集</li>
</ul>
<h1 id="GPU-Strategy"><a href="#GPU-Strategy" class="headerlink" title="GPU Strategy"></a>GPU Strategy</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>如何默认用全部 GPU 并且内存全部占满？</p>
<p>如何不浪费内存和计算资源?</p>
<ul>
<li>内存自增长</li>
<li>虚拟设备机制</li>
</ul>
<p>多 GPU 使用</p>
<ul>
<li>虚拟 GPU ； 实际 GPU</li>
<li>手工设置 ； 分布式机制</li>
</ul>
<p>API 列表（旧的 tensorflow API，<strong>可能已经过时</strong>）：</p>
<ul>
<li><code>tf.debugging.set log_device_placement</code> 某个变量分配在哪个设备上</li>
<li><code>tf.config.experimental.set_visible_devices</code> 本进程可见的设备</li>
<li><code>tf.config.experimental.ist logical_devices</code> 获取逻辑设备</li>
<li><code>tf.config.experimentalist physical_devices</code> 获取物理设备</li>
<li><code>tf.config.experimental.set_memory_growth</code> 用多少，设置多少</li>
<li><code>tf.config.experimental.VirtualDeviceConfiguration</code> 建立逻辑分区</li>
<li><code>tf.config.set_soft_device_placement</code> 自动把某个计算分配到某个设备上，不容易出错</li>
</ul>
<p><strong>使用多 GPU 的实际例子</strong>：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/tutorials/distribute/custom_training?hl=zh-cn">使用 tf.distribute.Strategy 进行自定义训练</a> ，分布式训练官方实例</li>
<li><a target="_blank" rel="noopener" href="https://tf.wiki/zh_hans/appendix/distributed.html">TensorFlow 分布式训练</a> ，多机</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/yjk13703623757/article/details/80956268">分布式TensorFlow多主机多GPU原理与实现</a> ，已在 github 相应仓库备份</li>
<li><a target="_blank" rel="noopener" href="https://github.com/otto248/tensorflow_cifar10_multiGPU">Cifar10 的多 GPU 代码参考</a></li>
</ul>
<h2 id="分布式策略"><a href="#分布式策略" class="headerlink" title="分布式策略"></a>分布式策略</h2><p>为什么需要分布式？ 数据量太大，模型太复杂。</p>
<p>本小节以 tensorflow 为例。</p>
<h3 id="MirroredStrategy"><a href="#MirroredStrategy" class="headerlink" title="MirroredStrategy"></a>MirroredStrategy</h3><p><strong>同步</strong>式分布式训练，适用于<strong>一机多卡</strong>情况。</p>
<p>每个 GPU 都有网络结构的所有参数，这些参数会被同步。</p>
<p>数据并行</p>
<ul>
<li>Batch 数据切为 N 份分给各个 GPU</li>
<li>梯度聚合然后更新给各个 GPU 上的参数</li>
</ul>
<h3 id="CentralStorageStrategy"><a href="#CentralStorageStrategy" class="headerlink" title="CentralStorageStrategy"></a>CentralStorageStrategy</h3><p>MirroredStrategy 的变种。参数不是在每个 GPU 上，而是存储在一个设备上。</p>
<p>除了更新参数的计算之外，计算是在所有 GPU 上并行的。</p>
<h3 id="MultiWorkerMirroredStrategy"><a href="#MultiWorkerMirroredStrategy" class="headerlink" title="MultiWorkerMirroredStrategy"></a>MultiWorkerMirroredStrategy</h3><p>类似于 MirroredStrategy，适用于<strong>多机多卡</strong>情况。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/31/w59RU8qsZIWAeh6.png" alt="106-53.png"></p>
<h3 id="TPUStrategy"><a href="#TPUStrategy" class="headerlink" title="TPUStrategy"></a>TPUStrategy</h3><p>与 MirroredStrategy 类似，使用在 TPU 上的策略。</p>
<h3 id="ParameterServerStrategy"><a href="#ParameterServerStrategy" class="headerlink" title="ParameterServerStrategy"></a>ParameterServerStrategy</h3><p><strong>异步</strong>分布式，更加适用于大规模分布式系统。</p>
<p>机器分为 Parameter Server 和 worker 两类：</p>
<ul>
<li>Parameter server 负责整合梯度，更新参数</li>
<li>Worker 负责计算，训练网络</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/07/31/9tXUglEMsfAqJwi.png" alt="106-54.png"></p>
<p>Tensorflow 一开始支持分布式的时候，便是这种 parameter server 架构。TensorFlow 一般将任务分为两类 job：一类叫参数服务器，<strong>parameter server</strong>，简称为 ps，用于存储可训练的参数变量 <code>tf.Variable</code>；一类就是普通任务，称为 <strong>worker</strong>，用于执行具体的计算。</p>
<h2 id="同步与异步的优劣"><a href="#同步与异步的优劣" class="headerlink" title="同步与异步的优劣"></a>同步与异步的优劣</h2><p><strong>异步可以避免短板效应</strong></p>
<ul>
<li>多机多卡（不同机器的显卡类型往往不一致）</li>
</ul>
<p><strong>异步的计算会增加模型的泛化能力</strong></p>
<ul>
<li>异步不是严格正确的，所以模型更容忍错误</li>
</ul>
<p><strong>同步可以避免过多的通信</strong></p>
<ul>
<li>一机多卡</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://dropsong.github.io">dropsong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://dropsong.github.io/posts/eeca0305.html">https://dropsong.github.io/posts/eeca0305.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dropsong.github.io" target="_blank">dropsong's</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/TensorFlow/">TensorFlow</a><a class="post-meta__tags" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><a class="post-meta__tags" href="/tags/CNN/">CNN</a><a class="post-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E5%8F%98%E6%8D%A2/">图像风格变换</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><a class="post-meta__tags" href="/tags/Embedding/">Embedding</a><a class="post-meta__tags" href="/tags/RNN/">RNN</a><a class="post-meta__tags" href="/tags/LSTM/">LSTM</a><a class="post-meta__tags" href="/tags/GPU/">GPU</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/05/16/4BTsNnSgIPFewCh.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://sway.office.com/s/earvb0OKBw38frLT/images/MlKmiZMe5C5Gf5" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sway.office.com/s/earvb0OKBw38frLT/images/MlKmiZMe5C5Gf5" alt="Thanks ᗜ ‸ ᗜ"/></a><div class="post-qr-code-desc">Thanks ᗜ ‸ ᗜ</div></li><li class="reward-item"><a href="https://sway.office.com/s/C4drow041G0kHpIc/images/tO2U2uk1DoetkI" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sway.office.com/s/C4drow041G0kHpIc/images/tO2U2uk1DoetkI" alt="₍ᐢ.ˬ.⑅ᐢ₎"/></a><div class="post-qr-code-desc">₍ᐢ.ˬ.⑅ᐢ₎</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/ffce18f.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/10/1MoBUm7KaexdWg5.jpg" onerror="onerror=null;src='/img/404me.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">win + debian 双系统</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/6f3f8819.html" title="人工智能导论"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://upload-bbs.miyoushe.com/upload/2024/06/18/312648482/da9b91db012284f25da262d61dc1fffd_3404581033726032348.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-03</div><div class="title">人工智能导论</div></div></a></div><div><a href="/posts/1d11fe8c.html" title="手写数字识别"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/09/06/bPumfncHzeh4raO.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-06</div><div class="title">手写数字识别</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/11/07/tEWlYGuVUqFvxw7.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">dropsong</div><div class="author-info__description">把你 TeriTeri 掉哦～(∠・ω< )⌒★</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">106</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">161</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dropsong"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">右下设置简繁切换。安卓和 linux 上代码无法正常缩进。图片均有备份，无法加载可联系博主恢复。部分资源需翻墙。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9D%E8%AF%86-TensorFlow"><span class="toc-number">1.</span> <span class="toc-text">初识 TensorFlow</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensorflow-keras"><span class="toc-number">2.</span> <span class="toc-text">Tensorflow-keras</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">2.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-keras%E5%81%9A%E5%88%86%E7%B1%BB"><span class="toc-number">2.2.</span> <span class="toc-text">tf_keras做分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-keras%E5%81%9A%E5%9B%9E%E5%BD%92"><span class="toc-number">2.3.</span> <span class="toc-text">tf_keras做回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">3.</span> <span class="toc-text">梯度消失、梯度爆炸</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="toc-number">3.1.</span> <span class="toc-text">问题描述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">3.2.</span> <span class="toc-text">批归一化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E6%94%B9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">更改激活函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dropout"><span class="toc-number">4.</span> <span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Wide-amp-Deep-%E6%A8%A1%E5%9E%8B%EF%BC%88%E6%8E%A8%E8%8D%90%E5%9C%BA%E6%99%AF%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">Wide &amp; Deep 模型（推荐场景）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2"><span class="toc-number">6.</span> <span class="toc-text">超参数搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Size"><span class="toc-number">6.1.</span> <span class="toc-text">Batch Size</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">6.2.</span> <span class="toc-text">学习率衰减</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E4%B8%8B%E9%99%8D%EF%BC%88discrete-staircase%EF%BC%89"><span class="toc-number">6.2.1.</span> <span class="toc-text">离散下降（discrete staircase）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E6%95%B0%E5%87%8F%E7%BC%93%EF%BC%88exponential-decay%EF%BC%89"><span class="toc-number">6.2.2.</span> <span class="toc-text">指数减缓（exponential decay）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E3%80%81%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2"><span class="toc-number">6.3.</span> <span class="toc-text">网格搜索、随机搜索</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tensorflow-%E5%9F%BA%E7%A1%80-API"><span class="toc-number">7.</span> <span class="toc-text">tensorflow 基础 API</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dataset"><span class="toc-number">8.</span> <span class="toc-text">dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#estimator"><span class="toc-number">9.</span> <span class="toc-text">estimator</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">10.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E5%9B%B0%E5%A2%83"><span class="toc-number">10.1.</span> <span class="toc-text">全连接层的困境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%80%9D%E8%B7%AF"><span class="toc-number">10.2.</span> <span class="toc-text">卷积的思路</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E8%BF%9E%E6%8E%A5"><span class="toc-number">10.2.1.</span> <span class="toc-text">局部连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E3%80%81%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="toc-number">10.2.2.</span> <span class="toc-text">卷积操作、参数共享</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">10.3.</span> <span class="toc-text">池化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%88%98"><span class="toc-number">10.4.</span> <span class="toc-text">实战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.</span> <span class="toc-text">迁移学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ResNet"><span class="toc-number">12.</span> <span class="toc-text">ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86"><span class="toc-number">12.1.</span> <span class="toc-text">基本知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%88%98-1"><span class="toc-number">12.2.</span> <span class="toc-text">实战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6"><span class="toc-number">13.</span> <span class="toc-text">卷积神经网络进阶</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet"><span class="toc-number">13.1.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGGNet"><span class="toc-number">13.2.</span> <span class="toc-text">VGGNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet-1"><span class="toc-number">13.3.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#InceptionNet"><span class="toc-number">13.4.</span> <span class="toc-text">InceptionNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MobileNet"><span class="toc-number">13.5.</span> <span class="toc-text">MobileNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">13.6.</span> <span class="toc-text">不同模型结构的对比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82"><span class="toc-number">14.</span> <span class="toc-text">卷积神经网络调参</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">14.1.</span> <span class="toc-text">不同的优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E6%83%85%E6%8F%90%E8%A6%81-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">14.1.1.</span> <span class="toc-text">前情提要 - 梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaGrad"><span class="toc-number">14.1.2.</span> <span class="toc-text">AdaGrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSProp"><span class="toc-number">14.1.3.</span> <span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam"><span class="toc-number">14.1.4.</span> <span class="toc-text">Adam</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">14.2.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid"><span class="toc-number">14.2.1.</span> <span class="toc-text">Sigmoid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh"><span class="toc-number">14.2.2.</span> <span class="toc-text">Tanh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU"><span class="toc-number">14.2.3.</span> <span class="toc-text">ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Leaky-ReLU"><span class="toc-number">14.2.4.</span> <span class="toc-text">Leaky-ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ELU"><span class="toc-number">14.2.5.</span> <span class="toc-text">ELU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Maxout"><span class="toc-number">14.2.6.</span> <span class="toc-text">Maxout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">14.2.7.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%88w%E5%92%8Cb%EF%BC%89"><span class="toc-number">14.3.</span> <span class="toc-text">网络初始化（w和b）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96-1"><span class="toc-number">14.4.</span> <span class="toc-text">批归一化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">14.5.</span> <span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E5%A4%9A%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7"><span class="toc-number">14.6.</span> <span class="toc-text">更多调参技巧</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%88%98-2"><span class="toc-number">14.7.</span> <span class="toc-text">实战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E5%8F%98%E6%8D%A2"><span class="toc-number">15.</span> <span class="toc-text">图像风格变换</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E5%81%9A%E4%BB%80%E4%B9%88"><span class="toc-number">15.1.</span> <span class="toc-text">神经网络可以做什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E5%88%B0%E4%BA%86%E4%BB%80%E4%B9%88"><span class="toc-number">15.2.</span> <span class="toc-text">卷积神经网络学到了什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2%EF%BC%88%E7%AE%80%E5%8D%95%E7%89%88%E6%9C%AC%EF%BC%89"><span class="toc-number">15.3.</span> <span class="toc-text">图像风格转换（简单版本）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%93%E5%B1%95"><span class="toc-number">15.4.</span> <span class="toc-text">拓展</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Embedding"><span class="toc-number">16.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RNN"><span class="toc-number">17.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C-LSTM"><span class="toc-number">18.</span> <span class="toc-text">长短期记忆网络 LSTM</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GPU-Strategy"><span class="toc-number">19.</span> <span class="toc-text">GPU Strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E5%85%A5"><span class="toc-number">19.1.</span> <span class="toc-text">引入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%AD%96%E7%95%A5"><span class="toc-number">19.2.</span> <span class="toc-text">分布式策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MirroredStrategy"><span class="toc-number">19.2.1.</span> <span class="toc-text">MirroredStrategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CentralStorageStrategy"><span class="toc-number">19.2.2.</span> <span class="toc-text">CentralStorageStrategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MultiWorkerMirroredStrategy"><span class="toc-number">19.2.3.</span> <span class="toc-text">MultiWorkerMirroredStrategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TPUStrategy"><span class="toc-number">19.2.4.</span> <span class="toc-text">TPUStrategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ParameterServerStrategy"><span class="toc-number">19.2.5.</span> <span class="toc-text">ParameterServerStrategy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5%E7%9A%84%E4%BC%98%E5%8A%A3"><span class="toc-number">19.3.</span> <span class="toc-text">同步与异步的优劣</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/eeca0305.html" title="深度学习小记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/16/4BTsNnSgIPFewCh.jpg" onerror="this.onerror=null;this.src='/img/404me.png'" alt="深度学习小记"/></a><div class="content"><a class="title" href="/posts/eeca0305.html" title="深度学习小记">深度学习小记</a><time datetime="2025-05-16T06:57:13.000Z" title="发表于 2025-05-16 14:57:13">2025-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/ffce18f.html" title="win + debian 双系统"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/05/10/1MoBUm7KaexdWg5.jpg" onerror="this.onerror=null;this.src='/img/404me.png'" alt="win + debian 双系统"/></a><div class="content"><a class="title" href="/posts/ffce18f.html" title="win + debian 双系统">win + debian 双系统</a><time datetime="2025-05-10T13:16:59.000Z" title="发表于 2025-05-10 21:16:59">2025-05-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/5888fb8e.html" title="微积分补完计划"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://upload-bbs.miyoushe.com/upload/2025/01/26/75276539/18ac78490f43bf9da7f8c453a59555b5_2308110768772033072.jpg" onerror="this.onerror=null;this.src='/img/404me.png'" alt="微积分补完计划"/></a><div class="content"><a class="title" href="/posts/5888fb8e.html" title="微积分补完计划">微积分补完计划</a><time datetime="2025-05-01T15:17:43.000Z" title="发表于 2025-05-01 23:17:43">2025-05-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/b1b54fd.html" title="机器学习笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/04/01/uzqBjab1yZW9HLU.jpg" onerror="this.onerror=null;this.src='/img/404me.png'" alt="机器学习笔记"/></a><div class="content"><a class="title" href="/posts/b1b54fd.html" title="机器学习笔记">机器学习笔记</a><time datetime="2025-04-01T07:38:47.000Z" title="发表于 2025-04-01 15:38:47">2025-04-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/60ce6585.html" title="拉格朗日插值算法实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://upload-bbs.miyoushe.com/upload/2024/10/18/285958508/948be2308701fbe302fcdd5d7045d43a_8244217296929491053.jpeg" onerror="this.onerror=null;this.src='/img/404me.png'" alt="拉格朗日插值算法实现"/></a><div class="content"><a class="title" href="/posts/60ce6585.html" title="拉格朗日插值算法实现">拉格朗日插值算法实现</a><time datetime="2025-03-23T15:47:53.000Z" title="发表于 2025-03-23 23:47:53">2025-03-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By dropsong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23lijmGQilj9J9CrdP',
      clientSecret: '255f80b6a1783219f05a3c250856e45342d37c3b',
      repo: 'dropsong.github.io',
      owner: 'dropsong',
      admin: ['dropsong'],
      id: '6e170c7517afa38a1dee5172e4e02973',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script></div><!-- hexo injector body_end start --><script async src="/js/ali_font.js"></script><!-- hexo injector body_end end --></body></html>