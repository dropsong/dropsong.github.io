<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>人工智能导论 | dropsong's</title><meta name="keywords" content="AI,A*,蒙特卡洛树,α-β剪枝,主观贝叶斯,机器学习,决策树,信息论,遗传算法,SVM,KNN,神经网络"><meta name="author" content="dropsong"><meta name="copyright" content="dropsong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="绪论    人工智能程序 通常计算机程序     主要是符号处理 主要是数字处理   启发式搜索 依靠算法   控制结构和知识域相分 信息和控制联结在一起   易于修改、更新和改变 难以修改   允许不正确的答案 要求正确的回答   AI程序：干什么 传统程序：干些什么及如何干     AI 概览：  AI、机器学习、深度学习的关系：   三大学派：符号主义、连接主义、行为主义。 符号主义（逻辑主">
<meta property="og:type" content="article">
<meta property="og:title" content="人工智能导论">
<meta property="og:url" content="https://dropsong.github.io/posts/6f3f8819.html">
<meta property="og:site_name" content="dropsong&#39;s">
<meta property="og:description" content="绪论    人工智能程序 通常计算机程序     主要是符号处理 主要是数字处理   启发式搜索 依靠算法   控制结构和知识域相分 信息和控制联结在一起   易于修改、更新和改变 难以修改   允许不正确的答案 要求正确的回答   AI程序：干什么 传统程序：干些什么及如何干     AI 概览：  AI、机器学习、深度学习的关系：   三大学派：符号主义、连接主义、行为主义。 符号主义（逻辑主">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://upload-bbs.miyoushe.com/upload/2024/06/18/312648482/da9b91db012284f25da262d61dc1fffd_3404581033726032348.jpg">
<meta property="article:published_time" content="2024-07-03T13:16:36.000Z">
<meta property="article:modified_time" content="2024-09-01T14:51:36.391Z">
<meta property="article:author" content="dropsong">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="A*">
<meta property="article:tag" content="蒙特卡洛树">
<meta property="article:tag" content="α-β剪枝">
<meta property="article:tag" content="主观贝叶斯">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="决策树">
<meta property="article:tag" content="信息论">
<meta property="article:tag" content="遗传算法">
<meta property="article:tag" content="SVM">
<meta property="article:tag" content="KNN">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://upload-bbs.miyoushe.com/upload/2024/06/18/312648482/da9b91db012284f25da262d61dc1fffd_3404581033726032348.jpg"><link rel="shortcut icon" href="/img/favicon2.jpg"><link rel="canonical" href="https://dropsong.github.io/posts/6f3f8819"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '人工智能导论',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-01 22:51:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="dropsong's" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/11/07/tEWlYGuVUqFvxw7.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">94</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">112</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://upload-bbs.miyoushe.com/upload/2024/06/18/312648482/da9b91db012284f25da262d61dc1fffd_3404581033726032348.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">dropsong's</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">人工智能导论</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2024-07-03T13:16:36.000Z" title="发表于 2024-07-03 21:16:36">2024-07-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.2k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">人工智能程序</th>
<th style="text-align:left">通常计算机程序</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">主要是<strong>符号</strong>处理</td>
<td style="text-align:left">主要是<strong>数字</strong>处理</td>
</tr>
<tr>
<td style="text-align:left"><strong>启发式搜索</strong></td>
<td style="text-align:left">依靠<strong>算法</strong></td>
</tr>
<tr>
<td style="text-align:left">控制结构和知识域<strong>相分</strong></td>
<td style="text-align:left">信息和控制联结在<strong>一起</strong></td>
</tr>
<tr>
<td style="text-align:left">易于修改、更新和改变</td>
<td style="text-align:left">难以修改</td>
</tr>
<tr>
<td style="text-align:left"><strong>允许不正确</strong>的答案</td>
<td style="text-align:left">要求<strong>正确的</strong>回答</td>
</tr>
<tr>
<td style="text-align:left">AI程序：<strong>干什么</strong></td>
<td style="text-align:left">传统程序：<strong>干些什么及如何干</strong></td>
</tr>
</tbody>
</table>
</div>
<p>AI 概览：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/03/bV5jKDqWYnBHiZS.png" alt="80-1.png"></p>
<p>AI、机器学习、深度学习的关系：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2024/07/03/668542c0e08db.png" alt="80-2.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/03/y4JTQjUlhdPKftY.png" alt="80-3.png"></p>
<p>三大学派：符号主义、连接主义、行为主义。</p>
<p><strong>符号主义（逻辑主义、心理学派、计算机学派）</strong></p>
<ul>
<li>原理：物理符号系统假设和有限合理性假设</li>
<li>起源：源于数理逻辑</li>
<li>基本思想<ul>
<li>人的认知基元是符号，智能和知识可用符号表示，认知过程即符号操作过程，擅长抽象思维。</li>
<li>人是一个物理符号系统，计算机也是一个物理符号系统，因此，能用计算机来模拟人的智能行为。</li>
<li>知识是信息的一种形式，是构成智能的基础。人工智能的核心问题是知识表示、知识推理。</li>
<li><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/03/g6Jr8otqR4DnTiE.png" alt="80-4.png"></li>
<li>启发式程序、专家系统、知识工程</li>
</ul>
</li>
</ul>
<p><strong>连接主义（生理学派）</strong></p>
<ul>
<li>原理：神经网络及神经网络间的连接机制和学习算法</li>
<li>起源：源于仿生学，特别是人脑模型的研究</li>
<li>基本思想<ul>
<li>认识的基本元素是神经元，认识过程是大量神经元的并行活动，擅长形象思维。</li>
<li>人脑不同于电脑，并提出连接主义的大脑工作模式，用于取代符号操作的电脑工作模式。</li>
<li><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/03/ZuGbtIgvWSicOxP.png" alt="80-5.png"></li>
</ul>
</li>
</ul>
<p><strong>行为主义（进化主义、控制论学派）</strong></p>
<ul>
<li>原理：控制论及感知-动作型控制系统</li>
<li>起源：源于控制论</li>
<li>基本思想<ul>
<li>智能取决于感知和行动，提出智能行为的“感知-动作”模式。</li>
<li>智能不需要知识、表示和推理；人工智能可以像人类智能一样逐步进化；智能行为只能在现实世界中与周围环境进行交互作用而表现出来。</li>
<li><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/03/Tmw8Loh2BAtVkRx.png" alt="80-6.png"></li>
</ul>
</li>
</ul>
<p><strong>三大学派比较</strong>：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/03/ZOA5g7RtHN6QEM4.png" alt="80-7.png"></p>
<h1 id="搜索技术"><a href="#搜索技术" class="headerlink" title="搜索技术"></a>搜索技术</h1><h2 id="搜索问题"><a href="#搜索问题" class="headerlink" title="搜索问题"></a>搜索问题</h2><p>搜索方式的分类：</p>
<ul>
<li>回溯搜索</li>
<li>盲目搜索（深度优先、宽度优先）</li>
<li>启发式搜索（<code>A</code>算法→<code>A*</code>算法→<code>A*</code>算法的改进）</li>
</ul>
<p>传教士和野人问题：<br>问题可以转化为<strong>状态空间的搜索问题</strong>：</p>
<ul>
<li>用在河的左岸的传教士人数、野人人数和船的情况表示问题</li>
<li>初始状态用三元组表示为（3，3，1）</li>
<li>结束状态为（0，0，0）</li>
<li>中间状态为（2，2，0）、（3，2，1）、（3，0，0）…… 等，每个三元组对应了三维空间上的一个点</li>
</ul>
<p><strong>表示方法 — 状态空间表示法</strong></p>
<ul>
<li><strong>状态</strong>用来表示系统状态,事实等叙述型知识的一组变量或数组<br>$Q = [q_1, q_2, …, q_n]^t$</li>
<li><strong>操作</strong>是用来表示引起状态变化的过程型知识的一组关系或函数<br>$F:\{f_1, f_2, …, f_m\}$</li>
<li><strong>状态空间(State Space)</strong> 是利用状态变量和操作符号，表示系统或问题的有关知识的符号体系<br>四元组 $(S, O, S_0, G):$<br>$S$ 状态集合<br>$O$ 操作算子集合<br>$S_0$ 初始状态，$S_0 \subset S$<br>$G$ 目的状态，$G \subset S$，（可是若干具体状态，也可是满足某些性质的路径信息描述）<br><strong>从S0结点到G结点的路径被称为求解路径。</strong></li>
<li><strong>状态空间的一个解</strong>是一有限操作算子序列，它使初始状态转为目标状态<br>$S_0 \xrightarrow{O_1} S_1 \xrightarrow{O_2} S_2 \xrightarrow{O_3} \cdots \xrightarrow{O_k} G$<br>其中 O1，…，Ok 即为状态空间的一个解（解往往不是唯一的）</li>
</ul>
<p>如何在一个较大的问题空间中，只搜索较小的范围，就找到问题的解呢？</p>
<p>对于大空间问题，搜索策略要考虑组合爆炸的问题。</p>
<p><strong>盲目搜索</strong>，未利用问题的知识，采用固定的方式生成状态的方法。</p>
<p><strong>启发式搜索</strong>，利用问题的知识，缩小问题的搜索范围，选择那些最有可能在（最优）解路径上的状态优先搜索，以尽快地找到问题的（最优）解。</p>
<h2 id="回溯策略"><a href="#回溯策略" class="headerlink" title="回溯策略"></a>回溯策略</h2><p><strong>考虑一个经典问题：N 皇后。</strong></p>
<p>我们可以用 dfs 的方法寻找解，但是效率不高。有什么改进的思路吗？</p>
<ul>
<li>回溯有时不是上一步造成的，有可能是更早的那一步造成的——多步回溯</li>
<li>找到回溯的原因，在开始就避免回溯——需要引入一些相关信息</li>
<li>当然相关信息的引入不应造成搜索负担的巨大加重，否则得不偿失</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/04/oNBmg2RPncKG9Xz.png" alt="80-8.png"></p>
<p>如此一来，相比于固定排序的搜索树（没有引入知识），动态排序的搜索树（引入知识）的回溯次数大大减少。</p>
<p>回溯搜索算法改进（？）：</p>
<ul>
<li>推广的回溯算法可应用于一般问题的求解，但这两个算法只描述了回溯一层的情况，即第 n 层递归调用失败，则控制退回到（n－1）层</li>
<li>深层搜索失败往往在于浅层原因，因此也可以利用启发信息，分析失败的原因，再回溯到合适的层次上，即<strong>多层回溯策略</strong>，目前已有一些系统使用了这种策略</li>
</ul>
<h2 id="图搜索策略"><a href="#图搜索策略" class="headerlink" title="图搜索策略"></a>图搜索策略</h2><p>问题的引出：</p>
<ul>
<li>回溯搜索：只保留从初始状态到当前状态的一条路径<ul>
<li>节省空间，但已搜索部分不能被以后使用</li>
</ul>
</li>
<li>图搜索：保留所有已经搜索过的路径<ul>
<li>搜索过的路径被保留</li>
<li>利用相关知识，可以进行启发式搜索</li>
</ul>
</li>
<li>图搜索策略是实现从一个隐含图中生成出一部分确实含有一个目标节点的显式表示子图的搜索过程</li>
</ul>
<p>略：一般的图搜索算法、dfs、bfs</p>
<h3 id="启发式图搜索"><a href="#启发式图搜索" class="headerlink" title="启发式图搜索"></a>启发式图搜索</h3><p>利用知识来引导搜索，减少搜索范围，降低问题复杂度。</p>
<p>启发信息的强度</p>
<ul>
<li>强：降低搜索量，但可能找不到最优解</li>
<li>弱：工作量较大，可能退化为盲目搜索，但找到最优解的概率相对较大</li>
</ul>
<h3 id="A-算法"><a href="#A-算法" class="headerlink" title="A* 算法"></a>A* 算法</h3><p>算法流程（如果链接挂了可在 archive.ph 中查看）：</p>
<p><a target="_blank" rel="noopener" href="https://paul.pub/a-star-algorithm/">https://paul.pub/a-star-algorithm/</a></p>
<p>很多介绍 A* 的文章只是讲了 how ，而没有讲 why ，少数讲 why 的博主恐怕在一些细节上也和自己和解了。为此，找一下原始的论文是必要的：</p>


	<div class="row">
		<iframe src="https://drive.google.com/file/d/1aRpKPbJ1qMwoRRWjSso47WKvTp5PKScE/preview" style="width:100%; height:550px"></iframe>
	</div>



<p>很好论文，解决了我多年的疑惑：</p>
<ul>
<li>如果 h(n) 始终小于等于节点 n 到终点的代价，则 A* 算法保证一定能够找到最短路径。为什么？</li>
<li>另外一个小细节：f(n) 是单调不减的，这意味着一旦节点从优先队列中被移出并处理，它的最优路径已经被找到，因为任何从此节点出发的新路径不会比已经找到的路径更优。</li>
</ul>
<p>练习：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.luogu.com.cn/problem/P5507">P5507 机关</a>：A*</li>
<li><a target="_blank" rel="noopener" href="https://www.luogu.com.cn/problem/P2324">SCOI2005 骑士精神</a>：IDA*</li>
</ul>
<h2 id="与或图搜索问题"><a href="#与或图搜索问题" class="headerlink" title="与或图搜索问题"></a>与或图搜索问题</h2>

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1ZC5ytKk3EpBHKfzWS6vl2-mvyWvtw1pZ/preview" style="width:100%; height:550px"></iframe>
	</div>



<blockquote>
<p>目录：<br>与或图搜索  2<br>博弈树搜索（MINI-MAX、α-β剪枝）  10<br>蒙特卡洛树搜索  25</p>
</blockquote>
<p>一些视频资料：</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
    <iframe src="https://player.bilibili.com/player.html?bvid=BV1AK411M7BB&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position:absolute; height: 100%; width: 100%;"> </iframe>
</div>

<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
    <iframe src="https://player.bilibili.com/player.html?bvid=BV14Z4y1A7J3&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position:absolute; height: 100%; width: 100%;"> </iframe>
</div>

<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
    <iframe src="https://player.bilibili.com/player.html?bvid=BV1JD4y1Q7mV&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position:absolute; height: 100%; width: 100%;"> </iframe>
</div>

<h1 id="知识表示"><a href="#知识表示" class="headerlink" title="知识表示"></a>知识表示</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><strong>选取知识表示的方法的因素</strong>：</p>
<ul>
<li>表示范围是否广泛<ul>
<li>要求表示内容范围广泛<ul>
<li>数理逻辑表示是一种广泛的知识表示办法，如果单纯用数字表示，则范围就有限</li>
</ul>
</li>
</ul>
</li>
<li>是否适于推理<ul>
<li>人工智能只能处理适合推理的知识表示<ul>
<li>数学模型适合推理，普通的数据库只能供浏览检索，但不适合推理</li>
</ul>
</li>
</ul>
</li>
<li>是否适于计算机处理<ul>
<li>计算机只能处理离散的、量化的字节流。用文字表述的知识和连续形式表示的知识（如微分方程）不适合计算机处理</li>
</ul>
</li>
<li>是否有高效的算法<ul>
<li>考虑到实用的性能，必须有高效的求解算法，知识表示才有意义</li>
</ul>
</li>
<li>能否表示不精确知识<ul>
<li>自然界的信息具有先天的模糊性和不精确性，能否表示不精确知识也是考虑的重要因素</li>
<li>许多知识表示方法往往要经过改造，如确定性方法、主观贝叶斯方法等对证据和规则引入了不确定性度量，就是为了表达不精确的知识</li>
</ul>
</li>
<li>能否模块化</li>
<li>知识和元知识能否用统一的形式表示<ul>
<li>知识和元知识是属于不同层次的知识，使用统一的表示方法可以使知识处理简单</li>
</ul>
</li>
<li><p>是否加入启发信息</p>
<ul>
<li>在已知的前提下，如何最快的推得所需的结论，以及如何才能推得最佳的结论，我们的认识往往是不精确的。因此，往往需要在元知识（控制知识）加入一些控制信息，也就是通常所说的启发信息</li>
</ul>
</li>
<li><p>过程性表示还是说明性表示</p>
<ul>
<li>说明性知识表示涉及细节少，抽象程度高，因此可靠性好，修改方便，但执行效率低</li>
<li>过程性知识表示的优缺点与说明性知识表示相反</li>
</ul>
</li>
<li>表示方法是否自然<ul>
<li>一般要尽量在表示方法的自然和使用效率之间取得平衡。例如，对于推理来说，PROLOG 比高级语言如 Visual C++ 自然，但显然牺牲了效率</li>
</ul>
</li>
</ul>
<p>同构与同态：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2024/07/17/6697d113cd9f8.png" alt="80-9.png"></p>
<p><strong>表示观</strong>是对于“什么是表示”这一基本问题的不同理解和采用的方法论，即指导知识表示的思想观点称为表示观。</p>
<p>人工智能领域关于知识表示的观点的<strong>争论焦点</strong>是常识的处理、表示与推理的关系等问题。</p>
<p><strong>认识论表示观</strong>：认为表示是对自然世界的表述，表示自身不显示任何智能行为。其唯一的作用就是携带知识。这意味着表示可以独立于启发式来研究。</p>
<p><strong>本体论（D.Lenta提出）</strong>：认为表示是对自然世界的一种近似，它规定了看待自然世界的方式。即一个约定的集合。表示只是描述了关心的一部分，逼真是不可能的。本体论主要解决的问题是：</p>
<ul>
<li>表示需对世界的某个部分给与特别的注意（聚集），而对世界的另外部分衰减，以求达到有效求解。</li>
<li>对世界可以采用不同的方式来记述。注重的不是“其语言形式，而是其内容”。此内容不是某些特定领域的特殊的专家知识，而是自然世界中那些具有普通意义的一般知识。</li>
<li>计算效率无疑是表示的核心问题之一。即有效地知识组织及与领域有关的启发式知识是其提高计算效率的手段。</li>
<li>推理是表示观中不可缺少的一部分。表示研究应与启发式搜索联系起来。认为不考虑推理的纯粹表示是不存在的。</li>
</ul>
<h2 id="表示方法"><a href="#表示方法" class="headerlink" title="表示方法"></a>表示方法</h2><h3 id="逻辑表示法"><a href="#逻辑表示法" class="headerlink" title="逻辑表示法"></a>逻辑表示法</h3><p>逻辑表示法的例子：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2024/07/21/669cc5b6232ef.png" alt="80-10.png"></p>
<p>图片示例：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/21/KNMO48VQH3RLUqp.png" alt="80-11.png"></p>
<h3 id="产生式规则表示法"><a href="#产生式规则表示法" class="headerlink" title="产生式规则表示法"></a>产生式规则表示法</h3>

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1RgISIhsiBYFPGJIB6gGe1riGEGX0BV5W/preview" style="width:100%; height:550px"></iframe>
	</div>



<h3 id="语义网络表示法"><a href="#语义网络表示法" class="headerlink" title="语义网络表示法"></a>语义网络表示法</h3><p>语义网络表示法和产生式表示法及谓词逻辑表示法之间有着对应的表示能力。</p>
<p>表示形式：</p>
<ul>
<li>谓词逻辑表示法，Relation（Object1，Object2）</li>
<li>语义网络表示法为（Object1，Relation，Object2）</li>
<li>语义网络中连接弧上的语义关系对应于逻辑表示法中的谓词关系</li>
</ul>
<p>一些例子：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/21/g8rQ3GCadWH7xj4.png" alt="80-12.png"></p>
<p>语义网络推理，<strong>相应的推理方法还不完善。</strong> 语义网络的推理过程主要有两种：</p>
<ul>
<li>继承</li>
<li>匹配</li>
</ul>
<h3 id="框架表示法"><a href="#框架表示法" class="headerlink" title="框架表示法"></a>框架表示法</h3><p>省流：刻板印象、模板、…</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/21/lIEq4viYLdzSuHJ.png" alt="80-13.png"></p>
<h3 id="脚本表示法"><a href="#脚本表示法" class="headerlink" title="脚本表示法"></a>脚本表示法</h3><p>脚本表示法是框架的特殊形式。</p>
<p>例子，医院的脚本：</p>
<blockquote>
<p><strong>开场条件：</strong></p>
<ol>
<li>病人有病。</li>
<li>病人的病需要找医生诊治。</li>
<li>病人有钱。</li>
<li>病人能够去医院。</li>
</ol>
<p><strong>角色：</strong> 病人、医生、护士。</p>
<p><strong>道具：</strong> 医院、挂号室、椅子、桌子、药方、药房、钱、药。</p>
<p><strong>场景：</strong><br>场景1 进入医院<br>（1）  人走进医院<br>（2）  病人挂号<br>（3）  病人在椅子上坐下等待看病<br>场景2  看病<br>（1）  病人进入医生的办公室<br>（2）  病人向医生所说病状<br>（3）  医生向病人解释病情<br>（4）  医生给病人开药方<br>场景3  交费<br>（1）  病人到交费处<br>（2）  病人递交药方<br>（3）  病人交钱<br>（4）  病人取回药方及收据<br>场景4  取药<br>（1）  病人到药房<br>（2）  病人递交药方<br>（3）  病人取药<br>场景5 离开<br>（1）  病人离开医院</p>
<p><strong>结果：</strong><br>1．病人看病了，明白了自己的病是怎么回事<br>2．病人花了钱，买了药<br>3．医生付出了劳动<br>4．医院的药品少了</p>
</blockquote>
<p>特点：</p>
<ul>
<li>比语义网络、框架等呆板</li>
<li>知识表达范围很窄</li>
<li>不适用于表达各种知识，但对实现构思好的特定知识非常有效</li>
</ul>
<h2 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h2><p>什么是知识图谱？</p>
<ul>
<li>知识图谱是 Google 用于增强其搜索引擎功能的知识库（Google 知识图谱，5 亿对象，35 亿事实关系）。</li>
<li>从学术的角度，<strong>“知识图谱本质上是语义网络（Semantic Network）的知识库”</strong>。</li>
<li>从实际应用的角度出发，可以简单地把知识图谱理解成<strong>多关系图（Multi-relational Graph）</strong>。<ul>
<li>多关系图一般包含多种类型的节点和多种类型的边</li>
<li>节点：概念、实体</li>
<li>边：关系</li>
</ul>
</li>
</ul>
<p>知识图谱旨在以结构化的形式描述客观世界中存在的概念、实体及其间的复杂关系。</p>
<ul>
<li>概念：对客观事物的概念化表示，如人、动物、组织机构</li>
<li>实体：客观世界中的具体事务，画家达芬奇、作品蒙娜丽莎</li>
<li>关系：描述概念、实体之间客观存在的关联</li>
</ul>
<p>一些具体表现：</p>
<ul>
<li>传统搜索引擎会返回包含用户搜索关键词的页面</li>
<li>知识图谱会返回<strong>知识卡片（Knowledge card）</strong>，为用户查询或返回答案中所包含的概念或实体提供详细的结构化摘要<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/22/Mo3g2fP75uBQYZy.png" alt="80-14.png"></li>
</ul>
<p>知识图谱经历了由人工和群体智慧构建，到面向互联网利用机器学习和信息抽取技术自动获取的过程。</p>
<p>维基百科是利用群体智能建立的互联网上至今最大的知识资源。</p>
<p>知识图谱的应用：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/22/QHdFvaNWloMiDT1.png" alt="80-15.png"></p>
<h1 id="逻辑推理"><a href="#逻辑推理" class="headerlink" title="逻辑推理"></a>逻辑推理</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p><strong>演绎推理：</strong></p>
<ul>
<li>从全称判断推出特称判断或单称判断的过程，即从一般到个别的推理</li>
<li>演绎推理中最常用的形式是<strong>三段论</strong>法（大前提、小前提、结论）</li>
<li>例如：<ul>
<li>所有的推理系统都是智能系统——一般的知识</li>
<li>专家系统是推理系统——个体的判断</li>
<li>所以，专家系统是智能系统——新判断    </li>
</ul>
</li>
<li>演绎推理没有增加新的知识</li>
</ul>
<p><strong>归纳推理：</strong></p>
<ul>
<li>从足够多的事例中归纳出一般性结论的推理过程，是一种<strong>从个别到一般的推理过程</strong></li>
<li>常用的归纳推理有<strong>简单枚举法</strong>和<strong>类比法</strong><ul>
<li><strong>枚举法归纳推理</strong>是由已观察到的事物都有某属性，而没有观察到相反的事例，从而推出某类事物都有某属性，推理过程为：<ul>
<li>S1 是 P，S2 是 P，…，Sn 是 P，<br>（S1,S2, …,Sn 是 S 类中的个别事物，在枚举中兼容）<br>  推出 S 都是  P </li>
</ul>
</li>
<li>枚举法归纳推理分<strong>完全归纳推理</strong>与<strong>不完全归纳推理</strong>。完全归纳推理是必然性推理，不完全推理得出的结论不具有必然性。</li>
<li>在两个或两类事物在许多属性上都相同的基础上，推出它们在其它属性上也相同，是<strong>类比法归纳推理</strong>。</li>
<li>类比法归纳可形式化地表示为：<br>A 具有属性a,b,c,d,e；B 具有属性a,b,c,d；推出 B 也具有属性e 。</li>
<li>类比法的可靠程度决定于两个或两类事物的相同属性与推出的那个属性之间的相关程度，相关程度越高，则类比法的可靠性就越高</li>
</ul>
</li>
<li>归纳推理增加了知识（在机器学习部分称为归纳学习）</li>
</ul>
<p><strong>默认推理</strong></p>
<ul>
<li>又称<strong>缺省推理</strong>，是在知识不完全的情况下假设某些条件已经具备，所进行的推理。</li>
<li>如：在条件A已成立的情况下，如果没有足够的证据能证明条件B不成立，则就默认B是成立的，并在此默认的前提下进行推理，推导出某个结论。</li>
<li>如果到某一时刻发现原先所作的默认不正确，则要撤消所作的默认以及由此默认推出的所有结论，重新按新情况进行推理。</li>
</ul>
<h2 id="归结原理"><a href="#归结原理" class="headerlink" title="归结原理"></a>归结原理</h2><p>Robinson 的归结原理使得自动定理证明得以实现。归结推理方法是机器定理证明的主要方法。</p>


	<div class="row">
		<iframe src="https://drive.google.com/file/d/1VsMFwZ5a0ZdOAVsMRdMRaB0GNQqeVdRM/preview" style="width:100%; height:550px"></iframe>
	</div>



<blockquote>
<p><strong>目录</strong><br>1 命题逻辑的归结法<br>10 谓词逻辑归结基础（SKolem标准型、子句集）<br>30 归结原理（置换、归结式、“快乐学生”问题）<br>50 归结过程的控制策略（删除、支撑集、语义、线性、单元、输入）</p>
</blockquote>
<h1 id="不确定性推理"><a href="#不确定性推理" class="headerlink" title="不确定性推理"></a>不确定性推理</h1><h2 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h2><p>不确定性包括<strong>证据</strong>的不确定性和<strong>知识</strong>的不确定性。</p>
<p>证据通常有两类：<strong>初始事实</strong>、推理过程中产生的<strong>中间结果</strong>。</p>
<p>证据的不确定性用 C(E) 表示。</p>
<p>在规则中，E 是规则的前提即证据，H 是该规则的结论，也可以是其它规则的证据。</p>
<ul>
<li>规则的不确定性：用一个数值 f(E，H) 表示，称为<strong>规则强度</strong>。</li>
</ul>
<p>在不精确推理中，由于知识和证据都具有不确定性，而且知识所要求的不确定性程度与证据实际具有的不确定性程度不一定相同，因而就出现了“怎样才算匹配成功？”的问题。</p>
<p>可以设计一个算法用来计算匹配双方<strong>相似的程度</strong>，另外再指定一个<strong>相似的限度</strong>，用来衡量匹配双方相似的程度是否落在指定的限度内。</p>
<p><strong>不确定性的更新和传播：</strong><br>在推理过程中如何考虑知识不确定性的动态积累和传递？</p>
<p>推理树：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/23/wHcWI12nh9aN3gR.png" alt="80-16.png"></p>
<h2 id="确定性方法（可信度方法）"><a href="#确定性方法（可信度方法）" class="headerlink" title="确定性方法（可信度方法）"></a>确定性方法（可信度方法）</h2><p>MYCIN 系统研制过程中产生的不确定推理方法（然而这个名字有点误导性）。</p>
<p>规则 A→B，其可信度 CF(B,A)，有 -1 ≤ CF(B, A) ≤ 1</p>
<script type="math/tex; mode=display">
CF(B,A)=
\begin{cases} 
\frac{P(B|A)-P(B)}{1-P(B)},  & \text{if }P(B|A) \ge P(B) \\
\frac{P(B|A)-P(B)}{P(B)}, & \text{if } P(B|A) \lt P(B)
\end{cases}</script><p>CF(B, A)表示的意义：证据为真时</p>
<ul>
<li>相对于 P(～B) = 1 - P(B) 来说，A 对 B 为真的支持程度。即 A 发生更支持 B 发生，此时 CF(B, A) ≥ 0</li>
<li>相对于 P(B) 来说，A 对 B 为真的不支持程度。即 A 发生不支持 B 发生，此时 CF(B, A) &lt; 0 </li>
</ul>
<p>CF(B, A)的特殊值：</p>
<ul>
<li>CF(B, A) = 1，前提真，结论必真</li>
<li>CF(B, A) = -1，前提真，结论必假</li>
<li>CF(B, A) = 0 ，前提真假与结论无关</li>
</ul>
<p><strong>实际应用中 CF(B, A) 的值由专家确定，并不是由P(B|A), P(B)计算得到的。</strong></p>
<p>证据 A 的可信度表示为 CF(A)，-1 ≤ CF( A) ≤ 1 .</p>
<p>特殊值：</p>
<ul>
<li>CF(A) = 1，    前提肯定真</li>
<li>CF(A) = -1，   前提肯定假</li>
<li>CF(A) = 0，       对前提一无所知</li>
<li>CF(A) ＞ 0， 表示A以CF(A)程度为真</li>
<li>CF(A) ＜ 0， 表示A以CF(A)程度为假</li>
</ul>
<p>实际使用时：</p>
<ul>
<li><strong>初始证据</strong>的 CF 值由专家根据经验提供</li>
<li><strong>其它证据</strong>的 CF 通过规则进行推理计算得到</li>
</ul>
<p><strong>“与”的计算</strong>：     A1 ∧ A2 →B<br>CF(A1 ∧ A2) = min { CF(A1), CF(A2) }</p>
<p><strong>“或”的计算</strong>：    A1 ∨ A2 →B<br>CF(A1 ∨ A2) = max { CF(A1), CF(A2) } </p>
<p><strong>“非”的计算：</strong><br>CF(～A) = -CF(A) </p>
<p><strong>由 A，A →B，求 B</strong>：<br>CF(B) = max{ 0, CF(A) } · CF(B,A)<br>（ CF(A) ＜ 0 时可以不算，即为“0”）</p>
<p><strong>合成</strong>：由 $CF_1(B)$、$CF_2(B)$，求 CF(B) </p>
<script type="math/tex; mode=display">
CF(B) = 
\begin{cases} 
CF_1(B)+CF_2(B)-CF_1(B)CF_2(B),  & CF_1(B) \ge 0, CF_2(B) \ge 0\\
CF_1(B)+CF_2(B)+CF_1(B)CF_2(B), & CF_1(B) \lt 0, CF_2(B) \lt 0\\
CF_1(B)+CF_2(B), & CF_1(B)CF_2(B) \lt 0
\end{cases}</script><p>注意：以上公式<strong>不满足组合交换性</strong>，即：计算结果与各条规则采用的先后顺序有关。</p>
<p>MYCIN 规定证据的<strong>可信度 CF(A)&lt;0.2 时，就认为该证据引入的规则不可使用。</strong></p>
<p>EMYCIN 系统（MYCIN 发展而成）对 CF1(B) 和 CF2(B) 符号不同时，进行了修正： </p>
<script type="math/tex; mode=display">
CF(B) = 
\begin{cases} 
CF_1(B)+CF_2(B)-CF_1(B)CF_2(B),  & CF_1(B) \ge 0, CF_2(B) \ge 0\\
CF_1(B)+CF_2(B)+CF_1(B)CF_2(B), & CF_1(B) \lt 0, CF_2(B) \lt 0\\
\frac{CF_1(B)+CF_2(B)}{1-\text{min}\{|CF_1(B)|, |CF_2(B)| \}} , & CF_1(B)CF_2(B) \lt 0
\end{cases}</script><p><strong>结论更新：</strong><br>已经有一个先验的结论可信度，如何由规则更新这个可信度？</p>
<p>结论更新的三种情况：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/31/kzcJFGyiX87eAs2.png" alt="80-17.png"></p>
<h2 id="主观-Bayes"><a href="#主观-Bayes" class="headerlink" title="主观 Bayes"></a>主观 Bayes</h2><p>该方法首先应用于地矿勘探系统 PROSPECTOR 中。</p>
<p>在这种方法中，引入了两个数值 <strong>（LS,LN）</strong> ：</p>
<ul>
<li>前者体现规则成立的充分性，后者则表现了规则成立的必要性</li>
<li>LS 表征的是 A 的发生对 B 发生的影响程度</li>
<li>LN 表征的是 A 的不发生对 B 发生的影响程度</li>
<li><strong>实际应用中，采用专家给定的 LS, LN 值</strong></li>
</ul>
<h3 id="证据的不确定性"><a href="#证据的不确定性" class="headerlink" title="证据的不确定性"></a>证据的不确定性</h3><p>几率函数 O(A) 表示证据 A 的不确定性：</p>
<script type="math/tex; mode=display">
O(A) = \frac{P(A)}{1-P(A)} = 
\begin{cases} 
0  & \text{A is false} \\
\infty  & \text{A is true} \\
(0,\infty) & \text{uncertain}
\end{cases}</script><p>几率函数与概率函数形式不同，但是变化相同：<strong>当A为真的程度越大（P(A)越大），几率函数的值也越大。</strong></p>
<p>在推理过程中需要概率函数值时，可用等式：</p>
<script type="math/tex; mode=display">
P(A) = \frac{O(A)}{1+O(A)}</script><p>特殊值：</p>
<ul>
<li>P(X) = 0, O(X) = 0</li>
<li>P(X) = 0.5, O(X) = 1</li>
<li>P(X) = 1, O(X) = inf</li>
</ul>
<h3 id="规则的不确定性"><a href="#规则的不确定性" class="headerlink" title="规则的不确定性"></a>规则的不确定性</h3><p>规则：A→B</p>
<script type="math/tex; mode=display">
P(B|A) = \frac{P(A|B)P(B)}{P(A)}</script><script type="math/tex; mode=display">
P(\lnot B|A) = \frac{P(A|\lnot B)P(\lnot B)}{P(A)}</script><p>两式相除：</p>
<script type="math/tex; mode=display">
\frac{P(B|A)}{P(\lnot B|A)} = \frac{P(A|B)P(B)}{P(A|\lnot B)P(\lnot B)}</script><p>记：</p>
<script type="math/tex; mode=display">
\frac{P(A|B)}{P(A|\lnot B)} = LS</script><p>又因为：</p>
<script type="math/tex; mode=display">
O(B) = \frac{P(B)}{P(\lnot B)} \qquad O(B|A) = \frac{P(B|A)}{P(\lnot B|A)}</script><p>整理得：</p>
<script type="math/tex; mode=display">
O(B|A) = LS \cdot O(B)</script><p><strong>LS 的含义</strong>：</p>
<ul>
<li>LS 表示 A 真对 B 的影响程度</li>
<li>LS = ∞ 时，P(~B|A)=0，P(B|A)=1</li>
<li>说明 A 对于 B 是逻辑充分的，即规则成立是充分的</li>
<li>LS 称作<strong>充分似然率因子</strong></li>
</ul>
<script type="math/tex; mode=display">
LS = \frac{O(B|A)}{O(B)} = \frac{P(B|A)/P(\lnot B|A)}{P(B)/P(\lnot B)}</script><p>LS 表示 A 存在对 B 发生的影响度：</p>
<ul>
<li>LS = 1，O(B|A) = O(B)，A 对 B 无影响</li>
<li>LS &gt; 1，O(B|A) &gt; O(B)，A 支持 B</li>
<li>LS &lt; 1，O(B|A) &lt; O(B)，A 不支持 B</li>
</ul>
<p>由类似的推导过程：</p>
<script type="math/tex; mode=display">
P(B|\lnot A) = \frac{P(\lnot A|B)P(B)}{P(\lnot A)}</script><script type="math/tex; mode=display">
P(\lnot B|\lnot A) = \frac{P(\lnot A|\lnot B)P(\lnot B)}{P(\lnot A)}</script><p>两式相除：</p>
<script type="math/tex; mode=display">
\frac{P(B|\lnot A)}{P(\lnot B| \lnot A)} = \frac{P(\lnot A|B)P(B)}{P(\lnot A| \lnot B)P(\lnot B)}</script><p>记：</p>
<script type="math/tex; mode=display">
\frac{P(\lnot A|B)}{P(\lnot A| \lnot B)} = LN</script><p>又因为：</p>
<script type="math/tex; mode=display">
O(B) = \frac{P(B)}{P(\lnot B)} \qquad O(B|\lnot A) = \frac{P(B|\lnot A)}{P(\lnot B|\lnot A)}</script><p>整理得：</p>
<script type="math/tex; mode=display">
O(B|\lnot A) = LN \cdot O(B)</script><p>LN 的含义：</p>
<ul>
<li>LN 表示 A 假(即不存在)对 B 的影响程度</li>
<li>LN = 0 时，P(B|~A)=0</li>
<li>说明 A 对于 B 是逻辑必要的，即规则成立是必要性</li>
<li>LN 称作<strong>必要似然率因子</strong></li>
</ul>
<script type="math/tex; mode=display">
LN = \frac{O(B|\lnot A)}{O(B)} = \frac{P(B|\lnot A)/P(\lnot B|\lnot A)}{P(B)/P(\lnot B)}</script><p>LN 表示 A 不存在对 B 发生的影响度：</p>
<ul>
<li>LN = 1，O(B|~A) = O(B)，~A对B无影响</li>
<li>LN &gt; 1，O(B|~A) &gt; O(B)，~A支持B</li>
<li>LN &lt; 1，O(B|~A) &lt; O(B)，~A不支持B</li>
</ul>
<p>LS、LN 的关系：<br><strong>LN≥0，LS≥0，且 LN 和 LS 彼此不独立。</strong></p>
<p>简单的验证：</p>
<script type="math/tex; mode=display">
LS = \frac{P(A|B)}{P(A|\lnot B)}
\qquad LN = \frac{P(\lnot A|B)}{P(\lnot A| \lnot B)}</script><p>当 LS &gt; 1 时：</p>
<script type="math/tex; mode=display">
P(A|B)>P(A|\lnot B)</script><p>故有：</p>
<script type="math/tex; mode=display">
LN = \frac{1-P(A|B)}{1-P(A|\lnot B)}<1</script><p>事实上，LS 和 LN 必处于下面三种情况之一：</p>
<ul>
<li>LS &gt; 1, LN &lt; 1</li>
<li>LS &lt; 1, LN &gt; 1</li>
<li>LS = LN = 1</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/31/RVHhLNKy2jWSnsB.png" alt="80-18.png"></p>
<p>LS、LN 的示例：</p>
<blockquote>
<p>“如果有石英矿，则必有钾矿带”。<br>LS=300, LN=0.2</p>
<p>这意味着：<br>发现石英矿，对判断发现钾矿带非常有利。而没有发现石英矿，并不暗示一定没有钾矿带。如果 LN &lt;&lt; 1，则没有发现石英矿时，强烈暗示钾矿带不存在。</p>
</blockquote>
<h3 id="推理计算"><a href="#推理计算" class="headerlink" title="推理计算"></a>推理计算</h3><p>给定先验几率、规则 LS 和 LN，怎样计算后验几率呢？</p>
<p><strong>1）</strong> A 必出现或必不出现时，即 P(A)=1 或 P(A)=0 时</p>
<script type="math/tex; mode=display">
O(B|A) = LS \cdot O(B)</script><script type="math/tex; mode=display">
O(B|\lnot A) = LN \cdot O(B)</script><p><strong>2）</strong> A 不是必出现或必不出现时，即 P(A)≠1 且 P(A)≠0 时</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/01/wxhtucPaoHR8ZJG.png" alt="80-19.png"></p>
<p>由全概率公式推得：</p>
<script type="math/tex; mode=display">
P(B|A') = P(B|A)P(A|A')+P(B|\lnot A)P(\lnot A|A')</script><p>对于上面的式子，先考虑三种特殊情况：</p>
<p><strong>『1』</strong> P(A|A’) = 1 时（证据 A’ 出现，证据 A 必然出现）</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(B|A') & = P(B|A) \\
        & = \frac{P(B|A)}{P(B|A)+P(\lnot B|A)} \\
        & = \frac{P(B|A)P(A)}{P(B|A)P(A)+P(\lnot B|A)P(A)} \\
        & = \frac{P(AB)}{P(AB)+P(\lnot BA)} \\
        & = \frac{P(AB)}{P(AB)+P(A|\lnot B)(1-P(B))} \\
        & = \frac{P(A|B)P(B)/P(A|\lnot B)}{P(A|B)P(B)/P(A|\lnot B)+1-P(B)} \\
        & = \frac{LS \cdot P(B)}{(LS-1) \cdot P(B)+1}
\end{aligned}</script><p><strong>『2』</strong> P(A|A’) = 0 时（证据 A’ 出现，证据 A 必然不出现）</p>
<p>类似地可以得到：</p>
<script type="math/tex; mode=display">
P(B|A') = P(B|\lnot A) = \frac{LN \cdot P(B)}{(LN-1) \cdot P(B)+1}</script><p><strong>『3』</strong> P(A|A’) = P(A) 时</p>
<p>显然有：</p>
<script type="math/tex; mode=display">
P(B| A') = P(B)</script><p><strong>综上『』</strong>，得到以下特殊值：</p>
<script type="math/tex; mode=display">
P(B| A') = 
\begin{cases}
\frac{LS \cdot P(B)}{(LS-1) \cdot P(B)+1}, & \text {P(A|A')=1} \\
\frac{LN \cdot P(B)}{(LN-1) \cdot P(B)+1}, &\text{P(A|A')=0} \\
P(B), &\text{P(A|A')=P(A)}
\end{cases}</script><p>在其他位置，使用插值计算：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/01/mIjMNJxkBdnXUEq.png" alt="80-20.png"></p>
<p>证据的组合、结论的合成：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/01/m4RlxZNfeK8BwhA.png" alt="80-21.png"></p>
<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h2><p>机器学习就是让机器（计算机）来模拟和实现人类的学习功能。</p>
<p>基本术语和概念：</p>
<ul>
<li><strong>数据集、训练集、测试集</strong><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/05/5o1MeumFAfdh7W4.png" alt="80-22.png"></li>
<li><strong>泛化能力、误差（训练误差、泛化误差）、欠拟合、过拟合</strong><ul>
<li>泛化能力好：对于新的未知数据，也能很好地预测结果。</li>
<li>过拟合：模型可以非常完美地拟合现有数据，但是对于新的数据，拟合效果不好。</li>
</ul>
</li>
<li>常见的机器学习任务<ul>
<li>分类</li>
<li>回归：线性回归、逻辑回归</li>
</ul>
</li>
<li>混淆矩阵<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/05/seyDVPGltc83pFg.png" alt="80-23.png"></li>
</ul>
<p>机器学习按<strong>是否有指导</strong>进行分类：</p>
<ul>
<li>监督学习</li>
<li>无监督学习</li>
<li>半监督学习</li>
<li>强化学习</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/06/l2sJZhnWqjzkQVE.png" alt="80-24.png"></p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>长这样：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/06/jGxvRt9JeYuNOHb.png" alt="80-25.png"></p>
<p>决策树学习的过程实际上是在构造决策树。<br>学习前提是必须有一组训练实例。<br>学习结果是根据训练实例构造的决策树。<br>学习完成后，就可以利用这棵决策树对未知事物进行分类。</p>
<p>可以利用多种算法构造决策树：</p>
<ul>
<li><code>ID 3</code></li>
<li><code>C4.5</code></li>
<li><code>CART</code></li>
<li><code>CHAID</code> </li>
</ul>
<p>ID3 算法是昆兰（J.R.Quinlan）于 1979 年提出的一种以<strong>信息熵的下降速度</strong>作为属性选择标准的一种学习算法。其输入是一个用来描述各种已知类别的例子集，学习结果是一棵用于进行分类的决策树。</p>
<p><strong>信息熵（information entropy）</strong>，是对信息源整体不确定性的度量。</p>
<p>The core idea of information theory is that the “informational value” of a communicated message depends on the degree to which the content of the message is surprising. If a highly likely event occurs, the message carries very little information. On the other hand, if a highly unlikely event occurs, the message is much more informative. For instance, the knowledge that some particular number will not be the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win. However, knowledge that a particular number will win a lottery has high informational value because it communicates the occurrence of a very low probability event.</p>
<p>The information content, also called the <i>surprisal</i> or <i>self-information</i>, of an event $E$ is a function which increases as the probability $p(E)$ of an event decreases. When $p(E)$ is close to 1, the surprisal of the event is low, but if $p(E)$ is close to 0, the surprisal of the event is high. This relationship is described by the function :</p>
<script type="math/tex; mode=display">
\log (\frac{1}{P(E)})</script><p>Hence, we can define the information, or surprisal, of an event $E$ by :</p>
<script type="math/tex; mode=display">
I(E)=−\log _2(p(E))</script><p>依据 Boltzmann’s H-theorem，香农把随机变量 X 的熵值 $\text{H}$（希腊字母Eta）定义如下：</p>
<script type="math/tex; mode=display">
\text{H} (X) = E[I(X)] = E[-\log p(X)]</script><p>其中 E 为期望函数。</p>
<p>当取自有限的样本时，熵的公式可以表示为：</p>
<script type="math/tex; mode=display">
\text{H} (X) = - \sum_{i} P(x_i)\log _b P(x_i)</script><p>当 b = 2 时，熵的单位是 bit .</p>
<p><strong>加权信息熵</strong>：<br>根据某种方法对样本 S 作一个划分：</p>
<script type="math/tex; mode=display">
S_1, S_2, \cdots , S_r</script><p>则划分后的加权信息熵为：</p>
<script type="math/tex; mode=display">
\sum _{t=1}^r \frac{|S_t|}{|S|} \text{H} (S_t)</script><p>假设 S 中的样本有 m 个属性，其属性集为 X={x1, x2,…,xm}，每个属性 xi 都有不同的取值（xi 取值的个数有 ri 种），根据这个 xi 的取值对样本 S 作划分，则可以记为：</p>
<script type="math/tex; mode=display">
\text{H} (S, x_i) = \sum _{t=1}^{r_i} \frac{|S_t|}{|S|} \text{H} (S_t)</script><p>其中 $S_t$ 为 $x_i = t$ 时的样本子集。</p>
<p>选取不同的属性进行划分，$\text{H} (S, x_i)$ 的值可能不同。记</p>
<script type="math/tex; mode=display">
G(S, x_i) = \text{H} (S) - \text{H} (S, x_i)</script><p>称为<strong>信息增益（information gain）</strong>。</p>
<p>ID3 算法的学习过程，实际上是一个以整个样本集为根节点，以信息增益最大为原则，选择条件属性进行扩展，逐步构造出决策树的过程。</p>
<p>一个粗略但简单的理解：寻找一个最优的划分。</p>
<p>例子：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/10/8GUDRfLJKbsIn2e.png" alt="80-26.png"></p>
<h2 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h2><p>遗传算法（GA）模拟自然界优胜劣汰的进化现象，把搜索空间映射为遗传空间，把可能的解编码成一个向量——染色体，向量的每个元素称为基因。 通过不断计算各染色体的适应值，选择最好的染色体，获得最优解。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/21/ewz1HWJra4huK8f.png" alt="80-27.png"></p>
<p>例子：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2024/08/21/66c5b6262dd53.png" alt="80-28.png"></p>
<p>交叉、变异：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/21/bNopUsOPtXEhrRn.png" alt="80-29.png"></p>
<p>遗传算法可用于 TSP 问题，具体细节略。</p>
<h2 id="SVM、KNN"><a href="#SVM、KNN" class="headerlink" title="SVM、KNN"></a>SVM、KNN</h2><p>支持向量机（support vector machine，SVM）是一个二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM还包括核技巧，这使它成为本质上的非线性分类器。支持向量机的学习算法是求解凸二次优化的最优化算法。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/21/Rf18v9mY4leLoit.png" alt="80-30.png"></p>
<p>K 最近邻（K-Nearest Neighbor，KNN）分类算法的思路是：如果一个样本在特征空间中的 K 个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/21/9yFbAOjewurDEzp.webp" alt="80-31.png"></p>
<h2 id="超参数、验证集"><a href="#超参数、验证集" class="headerlink" title="超参数、验证集"></a>超参数、验证集</h2><p>学习模型中一般有两种参数，一种参数是可以从学习中得到，还有一种无法数据里面得到，只能靠人的经验来设定，这类参数就叫做<strong>超参数</strong>。</p>
<p>模型超参数举例：</p>
<ul>
<li>神经网络的学习速率、迭代次数、批次大小、激活函数、神经元的数量</li>
<li>支持向量机的 C 和 σ</li>
<li>K 近邻中的 K</li>
</ul>
<p>超参数搜索的一般过程：</p>
<ul>
<li>将数据集分成训练集、验证集、测试集。</li>
<li>在训练集上根据模型的性能指标对模型参数进行优化。</li>
<li>在验证集上根据模型的性能指标对模型超参数进行搜索。</li>
<li>在步骤2和步骤3交替迭代进行，最终确定模型的参数和超参数，并在测试集中评价模型的优劣。</li>
</ul>
<p>超参数搜索算法：网格搜索、随机搜素、智能搜素、…</p>
<p>交叉验证：用来验证分类器的性能的一种统计分析方法，基本思想是把在某种意义下将原始数据进行分组，一部分作为训练集，另一部分作为验证集，首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型，以此作为评价分类器的性能指标。</p>
<p>k-折交叉验证（K-CV）：</p>
<ul>
<li>将原始数据分成k组（一般是均分）。</li>
<li>将每个子集数据分别做一次验证集，其余的k-1组子集数据作为训练集，这样会得到k个模型。</li>
<li>用这k个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标。</li>
</ul>
<h1 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>人工神经网络(Artificial Neural Network, ANN)，是一种旨在模仿人脑结构及其功能的信息处理系统。</p>
<p>神经元是神经网络的基本处理单元，科学研究过程中一般是用一个多输入/多输出的非线性器件来模拟生物神经细胞的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/27/ktN3uOoPJlbEx7f.png" alt="80-32.png"></p>
<p>在 M-P 模型中：</p>
<script type="math/tex; mode=display">
Y=\text{sign}[\sum_{i=1}^n x_iw_i-\theta]</script><p>$x_i$ 表示神经元的输入，$w_i$ 表示输入的对应权值，即信号源神经元与该神经元的连接强度，Y 为神经元的输出，$\theta$ 表示神经元的阈值。</p>
<p>深度学习一般指深度神经网络，这里的深度指神经网络的层数较多。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/27/wLY7B4uCVxeNHp2.png" alt="80-33.png"></p>
<p>激活函数设计一般需要考虑的因素：</p>
<ul>
<li>非线性</li>
<li>连续可微性</li>
<li>有界性</li>
<li>单调性</li>
<li>平滑性</li>
<li>原点附近近似 Identity</li>
</ul>
<p>基本模型：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/29/jsG7I5pnaFHitvW.png" alt="80-34.png"></p>
<h2 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h2><p>感知器基于 M-P 神经元模型。该模型由一个可调整权重的神经元(线性组合器)和一个硬限幅器组成。输入的加权和施加于硬限幅器，硬限幅器当其输入为正时输出为+1，输入为负时输出为-1（也可以是其他情况，具体由激活函数决定）。</p>
<p>一个基本感知器，用超平面将 n 维空间分为两个决策区域，超平面由线性分隔函数定义：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n x_iw_i-\theta =0</script><p><strong>用于分类任务的感知器训练算法：</strong></p>
<blockquote>
<p><strong>步骤1</strong>：初始化。设置权重w1, w2, … , wn和阈值 $\theta$ 的初值。初始权重可以随意赋值，取值范围通常为 [-0.5, 0.5] ，然后通过训练样本调整。<br><strong>步骤2</strong>：激活。通过用输入x1(p), x2(p), … ,xn(p)以及期望输出 $Y_d(p)$ 来激活感知器。迭代 p 时的实际输出为：</p>
<script type="math/tex; mode=display">
Y(p) = \text{step} [\sum_{i=1}^n x_i(p) w_i(p)-\theta]</script><p>其中，n 为感知器输入的数量，step 为阶跃激活函数。<br><strong>步骤3</strong>：权重训练。修改感知器的权重为：</p>
<script type="math/tex; mode=display">
w_i(p+1)= w_i(p)+ \Delta w_i(p)</script><p>其中， $\Delta w_i(p)=\alpha x_i(p)e(p)$ . e(p) 为误差。<br><strong>步骤4</strong>：迭代。迭代 p 加 1，回到步骤 2，重复以上过程直至收敛。</p>
</blockquote>
<p>例子：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/29/MbziVv8mXSfRyQT.png" alt="80-35.png"></p>
<p>感知器仅能学习线性分割函数，单层感知器只能解决线性分类问题。XOR 问题中无法找到一条合适的线进行分类。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/29/wciCtaYqWGQepk4.jpg" alt="80-36.jpg"></p>
<p>多层感知器可以解决非线性分类问题。</p>
<p>可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。</p>
<p>最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前 L−1 层看作表示，把最后一层看作线性预测器。这种架构通常称为<strong>多层感知机（multilayer perceptron）</strong>，通常缩写为 MLP。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/29/av3ZbfgGWhxYnqd.png" alt="80-37.png"></p>
<h2 id="梯度下降、损失函数"><a href="#梯度下降、损失函数" class="headerlink" title="梯度下降、损失函数"></a>梯度下降、损失函数</h2><p>当训练样例线性不可分时，我们无法找到一个超平面，令感知器完美分类训练样例，但是我们可以近似的分类它们，而允许一些小小的分类错误。怎样让这个错误最小呢，首先要参数化描述这个错误，这就是损失函数（误差函数），它反映了感知器目标输出和实际输出之间的错误。最常用的误差函数为 L2 误差：</p>
<script type="math/tex; mode=display">
E(w) = \frac{1}{2} \sum_{d \in D} (t_d - o_d)^2</script><p>其中，d 为训练样例，D 为训练样例集，$t_d$ 为目标输出，$o_d$ 为实际输出。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/29/DsAN2V3LhxyfiOX.png" alt="80-38.png"></p>
<p>梯度下降算法：</p>
<ul>
<li>全局梯度下降算法</li>
<li>随机梯度下降算法</li>
<li>Mini-Batch 梯度下降算法</li>
</ul>
<h2 id="BP-网络"><a href="#BP-网络" class="headerlink" title="BP 网络"></a>BP 网络</h2><p>BP 网络学习的网络基础是具有多层前馈结构的 BP 网络。为讨论方便，采用如下所示的三层 BP 网络。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/31/SXHWIuEBPcaU14h.png" alt="80-39.png"></p>
<p>对上述三层 BP 网络，分别用 i,j,k 表示输入层、隐含层、输出层节点，且以下符号表示：</p>
<ul>
<li>$O_i$, $O_j$, $O_k$ 分别表示输入层节点 i、隐含层节点 j，输出层节点 k 的输出；</li>
<li>$I_i$, $I_j$ ,$I_k$ 分别表示输入层节点 i、隐含层节点 j，输出层节点 k 的输入；</li>
<li>$w_{ij}$, $w_{jk}$ 分别表示从输入层节点 i 到隐含层节点 j ，从隐含层节点 j 输出层节点 k 的输入节点 j 的连接权值；</li>
<li>$θ_j$ 、$θ_k$ 分别表示隐含层节点 j、输出层节点 k 的阈值。</li>
</ul>
<p>计算过程和之前大体相当。</p>
<p>BP 网络的激发函数通常采用连续可微的函数，例如 sigmoid 函数：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{1+e^{-x}}</script><p>其一阶导数为：</p>
<script type="math/tex; mode=display">
f'(x) = f(x) [1-f(x)]</script><p>BP 网络学习过程是一个对给定训练模式，利用传播公式，沿着减小误差的方向不断调整网络联结权值和阈值的过程。</p>
<p>设样本集中的第 r 个样本，其输出层结点 k 的期望输出用 $d_{rk}$ 表示，实际输出用 $y_{rk}$ 表示。其中，$d_{rk}$ 由训练模式给出，且</p>
<script type="math/tex; mode=display">
y_{rk} = O_{rk}</script><p>如果仅针对单个输入样本，其实际输出与期望输出的误差为 </p>
<script type="math/tex; mode=display">
E = \frac{1}{2} \sum_{k=1}^l (d_k - y_k)^2</script><p>上述误差定义是针对单个训练样本的误差计算公式，它适用于网络的顺序学习方式。若采用批处理学习方式，需要定义其总体误差。假设样本集中有 R 个样本，则对整个样本集的总体误差定义为：</p>
<script type="math/tex; mode=display">
E_R = \sum_{r=1}^R E_r = \frac{1}{2} \sum_{r=1}^R \sum_{k=1}^l (d_{rk} - y_{rk})^2</script><p>针对顺序学习方式，其联结权值的调整公式为 </p>
<script type="math/tex; mode=display">
w_{jk}(t+1) = w_{jk}(t) + \Delta w_{jk}</script><p>式中，$w_{jk}(t)$ 和 $w_{jk}(t+1)$ 分别是第 t 次迭代和 t+1 次迭代时，从结点 j 到结点 k 的联结权值；$\Delta w_{jk}$ 是联结权值的变化量。</p>
<p>在开始推导之前，补充一点知识：</p>


	<div class="row">
    <embed src="https://dezeming.top/wp-content/uploads/2021/07/%E5%87%BD%E6%95%B0%E5%AF%B9%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC-%E9%80%9A%E4%BF%97%E6%98%93%E6%87%82%E7%9A%84%E6%8F%8F%E8%BF%B0.pdf" width="100%" height="550" type="application/pdf">
	</div>



<p>其实下面的推导只会用到这个 pdf 最开始的概念。多数情况下可以非常粗糙地理解——只要记住某些符号其实代表了一个向量，而每个向量都有很多元素，但是都可以用同一种方法计算，因而将它们记成了向量。</p>
<p>为了使联结权值能沿着 E 的梯度下降的方向逐渐改善，网络逐渐收敛，权值变化量 $\Delta w_{jk}$ 的计算公式如下：</p>
<script type="math/tex; mode=display">
\Delta w_{jk} = - \eta \frac{\partial E}{\partial w_{jk}}</script><p>式中，$\eta$ 为增益因子，取 [0, 1] 区间的一个正数，其取值与算法的收敛速度有关。</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w_{jk}} = \frac{\partial E}{\partial I_k} \frac{\partial I_k}{\partial w_{jk}}</script><p>注：E 是一个复合函数，这里用了链式法则。这样拆分是有实际意义的，比较显然。</p>
<script type="math/tex; mode=display">
\frac{\partial I_k}{\partial w_{jk}} = \frac{\partial}{\partial w_{jk}} \sum_{j=1}^m w_{jk} O_{j} = O_j</script><p>令局部梯度</p>
<script type="math/tex; mode=display">
\delta _k = - \frac{\partial E}{\partial I_k}</script><p>注意，这里 $\delta _k$ 代表了 $l$ 个变量，它们具有类似的形式。</p>
<p>那么有</p>
<script type="math/tex; mode=display">
\Delta w_{jk} = -\eta \frac{\partial E}{\partial I_k} \frac{\partial I_k}{\partial w_{jk}} = \eta \delta _k O_j</script><p><strong>对于输出层上的结点</strong>，则有 $O_k=y_k$ ，因此（这里 $y_k$ 也是向量）</p>
<script type="math/tex; mode=display">
\delta _k = - \frac{\partial E}{\partial I_k} = - \frac{\partial E}{\partial y_k} \frac{\partial y_k}{\partial I_k}</script><p>得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E}{\partial y_k}
&= \frac{\partial}{\partial y_k} ( \frac{1}{2} \sum_{i=1}^l (d_i - y_i)^2 )  \\
&= \frac{1}{2} \cdot 2 (d_k - y_k) \cdot (-1)  \\
&= -(d_k - y_k)
\end{aligned}</script><p>又因为</p>
<script type="math/tex; mode=display">
\frac{\partial y_k}{\partial I_k} = f'(I_k - \theta)</script><p>所以</p>
<script type="math/tex; mode=display">
\delta _k = (d_k - y_k) f'(I_k - \theta)</script><p>又因为 $f(I_k - \theta) = y_k$ ，再由 sigmoid 函数的导数的公式，有</p>
<script type="math/tex; mode=display">
\delta _k = (d_k - y_k) y_k (1-y_k)</script><p>代入之前的结果：</p>
<script type="math/tex; mode=display">
\Delta w_{jk} = \eta (d_k - y_k) (1 - y_k) y_k O_j</script><p>那么，<strong>对输出层有</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{jk}(t+1) 
&= w_{jk}(t) + \Delta w_{jk} \\
&= w_{jk}(t) + \eta (d_k - y_k) (1-y_k) y_k O_j \\
\end{aligned}</script><p><strong>现在，我们想要类似地计算 $\Delta w_{ij}$</strong> 。</p>
<p><strong>对于不是输出层的结点</strong>。它表示联结权值是作用于隐含层上的结点，我们可以类似地定义 $\delta _j$ ：</p>
<script type="math/tex; mode=display">
\delta _j = - \frac{\partial E}{\partial I_j} = - \frac{\partial E}{\partial O_j} \frac{\partial O_j}{\partial I_j}</script><p>容易得到：</p>
<script type="math/tex; mode=display">
\delta _j = - \frac{\partial E}{\partial O_j} f'(I_j - \theta _j)</script><p>为了便于理解，下面的 $O_j$ 可以理解为隐藏层中的随意一个节点的输出值（实际上它是向量）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
-\frac{\partial E}{\partial O_j}
&= - \frac{\partial E}{\partial I_k} \frac{\partial I_k}{\partial O_j} \\
&= (-\frac{\partial E}{\partial I_k}) \cdot \frac{\partial}{\partial O_j} \sum_{j=1}^m w_{jk} O_j \\
&= (- \frac{\partial E}{\partial I_k}) w_{jk} \\
\end{aligned}</script><p>上面式子最后结果中的 $w_{jk}$ 是指该 $O_j$ 对应的权值。</p>
<p>这个结果非常的 amazing 啊，注意到 </p>
<script type="math/tex; mode=display">
\delta _k = - \frac{\partial E}{\partial I_k}</script><p>于是有：</p>
<script type="math/tex; mode=display">
-\frac{\partial E}{\partial O_j} = \delta _k w_{jk}</script><p>于是</p>
<script type="math/tex; mode=display">
\delta _j = \delta _k w_{jk} f'(I_j - \theta _j)</script><p>这说明，低层结点的 $\delta$ 值是通过上一层结点的 $\delta$ 值来计算的。这样，我们就可以先计算出输出层上的 $\delta$ 值，然后在较低层上计算。</p>
<script type="math/tex; mode=display">
\delta _j = \delta _k w_{jk} f(I_j - \theta _j) [1 - f(I_j - \theta _j)]</script><p>于是</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta w_{ij}
&= \eta \delta _k w_{jk} f(I_j - \theta _j) [1 - f(I_j - \theta _j)] O_i \\
&= \eta \delta _k w_{jk} O_j (1 - O_j) x_i
\end{aligned}</script><p>那么，<strong>对隐含层有</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{ij}(t+1)
&= w_{ij}(t) + \Delta w_{ij} \\
&= \text{代入即可} \cdots \\
\end{aligned}</script><p>BP 网络的算法流程：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/09/01/eMNB6CQ5RDiAY2j.png" alt="80-40.png"></p>
<h2 id="拓展内容"><a href="#拓展内容" class="headerlink" title="拓展内容"></a>拓展内容</h2>

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1oP-LaYqY_X6sxByJhi9-6LvR6xjNcQYN/preview" style="width:100%; height:550px"></iframe>
	</div>



<blockquote>
<p><strong>目录</strong><br>卷积神经网络(CNN)  2<br>RNN  6<br>LSTM结构  9<br>注意力机制  11<br>正则化与优化器  18</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://dropsong.github.io">dropsong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://dropsong.github.io/posts/6f3f8819.html">https://dropsong.github.io/posts/6f3f8819.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dropsong.github.io" target="_blank">dropsong's</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/A/">A*</a><a class="post-meta__tags" href="/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91/">蒙特卡洛树</a><a class="post-meta__tags" href="/tags/%CE%B1-%CE%B2%E5%89%AA%E6%9E%9D/">α-β剪枝</a><a class="post-meta__tags" href="/tags/%E4%B8%BB%E8%A7%82%E8%B4%9D%E5%8F%B6%E6%96%AF/">主观贝叶斯</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a><a class="post-meta__tags" href="/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/">信息论</a><a class="post-meta__tags" href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/">遗传算法</a><a class="post-meta__tags" href="/tags/SVM/">SVM</a><a class="post-meta__tags" href="/tags/KNN/">KNN</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="https://upload-bbs.miyoushe.com/upload/2024/06/18/312648482/da9b91db012284f25da262d61dc1fffd_3404581033726032348.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://sway.office.com/s/earvb0OKBw38frLT/images/MlKmiZMe5C5Gf5" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sway.office.com/s/earvb0OKBw38frLT/images/MlKmiZMe5C5Gf5" alt="Thanks ᗜ ‸ ᗜ"/></a><div class="post-qr-code-desc">Thanks ᗜ ‸ ᗜ</div></li><li class="reward-item"><a href="https://sway.office.com/s/C4drow041G0kHpIc/images/tO2U2uk1DoetkI" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sway.office.com/s/C4drow041G0kHpIc/images/tO2U2uk1DoetkI" alt="₍ᐢ.ˬ.⑅ᐢ₎"/></a><div class="post-qr-code-desc">₍ᐢ.ˬ.⑅ᐢ₎</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/9e907e48.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2024/07/30/66a7c5df7cb0b.jpg" onerror="onerror=null;src='/img/404me.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">概率论与数理统计笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/7d04e5a3.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/07/01/a3Yf69FixPcNWDG.webp" onerror="onerror=null;src='/img/404me.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">读书笔记 23</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/1d11fe8c.html" title="手写数字识别"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/09/06/bPumfncHzeh4raO.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-06</div><div class="title">手写数字识别</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/11/07/tEWlYGuVUqFvxw7.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">dropsong</div><div class="author-info__description">把你 TeriTeri 掉哦～(∠・ω< )⌒★</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">94</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">112</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dropsong"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">右下设置简繁切换。安卓和 linux 上代码无法正常缩进。图片均有备份，无法加载可联系博主恢复。部分资源需翻墙。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">绪论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E6%8A%80%E6%9C%AF"><span class="toc-number">2.</span> <span class="toc-text">搜索技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.</span> <span class="toc-text">搜索问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E6%BA%AF%E7%AD%96%E7%95%A5"><span class="toc-number">2.2.</span> <span class="toc-text">回溯策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5"><span class="toc-number">2.3.</span> <span class="toc-text">图搜索策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E5%9B%BE%E6%90%9C%E7%B4%A2"><span class="toc-number">2.3.1.</span> <span class="toc-text">启发式图搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E7%AE%97%E6%B3%95"><span class="toc-number">2.3.2.</span> <span class="toc-text">A* 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E%E6%88%96%E5%9B%BE%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98"><span class="toc-number">2.4.</span> <span class="toc-text">与或图搜索问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.</span> <span class="toc-text">知识表示</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">表示方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E8%A1%A8%E7%A4%BA%E6%B3%95"><span class="toc-number">3.2.1.</span> <span class="toc-text">逻辑表示法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A7%E7%94%9F%E5%BC%8F%E8%A7%84%E5%88%99%E8%A1%A8%E7%A4%BA%E6%B3%95"><span class="toc-number">3.2.2.</span> <span class="toc-text">产生式规则表示法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E6%B3%95"><span class="toc-number">3.2.3.</span> <span class="toc-text">语义网络表示法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6%E8%A1%A8%E7%A4%BA%E6%B3%95"><span class="toc-number">3.2.4.</span> <span class="toc-text">框架表示法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%84%9A%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%B3%95"><span class="toc-number">3.2.5.</span> <span class="toc-text">脚本表示法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1"><span class="toc-number">3.3.</span> <span class="toc-text">知识图谱</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86"><span class="toc-number">4.</span> <span class="toc-text">逻辑推理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="toc-number">4.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E7%BB%93%E5%8E%9F%E7%90%86"><span class="toc-number">4.2.</span> <span class="toc-text">归结原理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%8E%A8%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">不确定性推理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-2"><span class="toc-number">5.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%96%B9%E6%B3%95%EF%BC%88%E5%8F%AF%E4%BF%A1%E5%BA%A6%E6%96%B9%E6%B3%95%EF%BC%89"><span class="toc-number">5.2.</span> <span class="toc-text">确定性方法（可信度方法）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A7%82-Bayes"><span class="toc-number">5.3.</span> <span class="toc-text">主观 Bayes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%81%E6%8D%AE%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7"><span class="toc-number">5.3.1.</span> <span class="toc-text">证据的不确定性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%84%E5%88%99%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7"><span class="toc-number">5.3.2.</span> <span class="toc-text">规则的不确定性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E8%AE%A1%E7%AE%97"><span class="toc-number">5.3.3.</span> <span class="toc-text">推理计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.</span> <span class="toc-text">机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-3"><span class="toc-number">6.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.2.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"><span class="toc-number">6.3.</span> <span class="toc-text">遗传算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E3%80%81KNN"><span class="toc-number">6.4.</span> <span class="toc-text">SVM、KNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E3%80%81%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">6.5.</span> <span class="toc-text">超参数、验证集</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text">人工神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">7.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="toc-number">7.2.</span> <span class="toc-text">感知器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">7.3.</span> <span class="toc-text">梯度下降、损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP-%E7%BD%91%E7%BB%9C"><span class="toc-number">7.4.</span> <span class="toc-text">BP 网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%93%E5%B1%95%E5%86%85%E5%AE%B9"><span class="toc-number">7.5.</span> <span class="toc-text">拓展内容</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/70707e46.html" title="工程数学"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/11/24/k3epdXnt48BSvTV.jpg" onerror="this.onerror=null;this.src='/img/404me.png'" alt="工程数学"/></a><div class="content"><a class="title" href="/posts/70707e46.html" title="工程数学">工程数学</a><time datetime="2024-11-24T14:15:15.000Z" title="发表于 2024-11-24 22:15:15">2024-11-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/b9315374.html" title="杂项笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2024/11/10/6730cce7808a4.png" onerror="this.onerror=null;this.src='/img/404me.png'" alt="杂项笔记"/></a><div class="content"><a class="title" href="/posts/b9315374.html" title="杂项笔记">杂项笔记</a><time datetime="2024-11-10T14:22:04.000Z" title="发表于 2024-11-10 22:22:04">2024-11-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/f33a0f8e.html" title="电网的分形层析成像"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/11/29/PvIFprWkyfDqmZb.webp" onerror="this.onerror=null;this.src='/img/404me.png'" alt="电网的分形层析成像"/></a><div class="content"><a class="title" href="/posts/f33a0f8e.html" title="电网的分形层析成像">电网的分形层析成像</a><time datetime="2024-11-04T04:29:37.000Z" title="发表于 2024-11-04 12:29:37">2024-11-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2ab4a7a3.html" title="DRF Note"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://bu.dusays.com/2024/10/31/672384b1ee617.png" onerror="this.onerror=null;this.src='/img/404me.png'" alt="DRF Note"/></a><div class="content"><a class="title" href="/posts/2ab4a7a3.html" title="DRF Note">DRF Note</a><time datetime="2024-10-31T13:13:48.000Z" title="发表于 2024-10-31 21:13:48">2024-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/eadf1d9.html" title="Python 高级"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://blog.likesrt.com/themes/theme-oyiso/assets/images/cover-post.jpg" onerror="this.onerror=null;this.src='/img/404me.png'" alt="Python 高级"/></a><div class="content"><a class="title" href="/posts/eadf1d9.html" title="Python 高级">Python 高级</a><time datetime="2024-10-22T12:41:29.000Z" title="发表于 2024-10-22 20:41:29">2024-10-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By dropsong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23lijmGQilj9J9CrdP',
      clientSecret: '255f80b6a1783219f05a3c250856e45342d37c3b',
      repo: 'dropsong.github.io',
      owner: 'dropsong',
      admin: ['dropsong'],
      id: '6bb1fcc2fefca48080edb5876395f36b',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script></div><!-- hexo injector body_end start --><script async src="/js/ali_font.js"></script><!-- hexo injector body_end end --></body></html>